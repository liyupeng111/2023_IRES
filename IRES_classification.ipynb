{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c401358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, convert_to_tensor, string\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow import linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, MaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.layers import Activation, Flatten, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import softmax\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Parameters for plotting model results ###\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.labelpad'] = 5\n",
    "plt.rcParams['axes.linewidth']= 2\n",
    "plt.rcParams['xtick.labelsize']= 14\n",
    "plt.rcParams['ytick.labelsize']= 14\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.minor.size'] = 3\n",
    "plt.rcParams['ytick.minor.size'] = 3\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6aeaa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a7a6748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "from functions import LRScheduler, TransformerModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50640d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.3.4\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version: ' + tf. __version__)\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83e7598e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic random seed\n",
    "DEFAULT_RANDOM_SEED = 2022\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# tensorflow random seed \n",
    "def seedTF(seed=DEFAULT_RANDOM_SEED):\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "# import torch\n",
    "# def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + tensorflow + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTF(seed)\n",
    "    # seedTorch(seed)\n",
    "\n",
    "seedEverything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32590485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model10\n",
      "fold1\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 600)]             0         \n",
      "_________________________________________________________________\n",
      "transformer_model (Transform (None, 600, 16)           15120     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                307232    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 323,441\n",
      "Trainable params: 313,841\n",
      "Non-trainable params: 9,600\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5010/5010 [==============================] - 1129s 225ms/step - loss: 0.2949 - tp: 47497.0000 - fp: 2332.0000 - tn: 156574.0000 - fn: 114193.0000 - accuracy: 0.6365 - precision: 0.9532 - recall: 0.2938 - auc: 0.8944 - prc: 0.8876 - val_loss: 0.0238 - val_tp: 34.0000 - val_fp: 155.0000 - val_tn: 7515.0000 - val_fn: 32.0000 - val_accuracy: 0.9758 - val_precision: 0.1799 - val_recall: 0.5152 - val_auc: 0.8747 - val_prc: 0.1264\n",
      "Epoch 2/10\n",
      " 163/5010 [..............................] - ETA: 18:17 - loss: 0.2067 - tp: 3712.0000 - fp: 166.0000 - tn: 4941.0000 - fn: 1613.0000 - accuracy: 0.8295 - precision: 0.9572 - recall: 0.6971 - auc: 0.9565 - prc: 0.9451"
     ]
    }
   ],
   "source": [
    "channels = [1, 4, 1, 1, 4, 1, 1, 1, 1, 1]\n",
    "\n",
    "for i in range(10,11):\n",
    "    model_name='model' + str(i)\n",
    "    print(model_name)\n",
    "\n",
    "    channel = channels[i-1]\n",
    "    if channel == 4:\n",
    "        padding_method = 'one_hot_encode_padding'\n",
    "    if channel ==1:\n",
    "        padding_method = 'encode_padding'\n",
    "\n",
    "    max_input_length = 600\n",
    "    dropout_rate = 0.1\n",
    "    feature_pad_end= '3end'\n",
    "\n",
    "    import models\n",
    "    custom_model = getattr(models, model_name)\n",
    "    import functions\n",
    "    custom_padding = getattr(functions, padding_method)\n",
    "    \n",
    "    for fold in range(1,6):\n",
    "        print('fold'+str(fold))\n",
    "        \n",
    "        data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "        data_train=data_all[data_all['set']=='train']\n",
    "        data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "        data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "        data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        X_train=custom_padding(data_train, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "        y_train=data_train['label']\n",
    "        X_valid=custom_padding(data_valid, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "        y_valid=data_valid['label']\n",
    "    \n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model=custom_model(seq_length=max_input_length, dropout_rate=dropout_rate)\n",
    "\n",
    "        #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "        adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "        METRICS = [\n",
    "              keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "              keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "              keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "              keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "              keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "              keras.metrics.Precision(name='precision'),\n",
    "              keras.metrics.Recall(name='recall'),\n",
    "              keras.metrics.AUC(name='auc'),\n",
    "              keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "        ]\n",
    "\n",
    "        model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                  batch_size=64, epochs=10, verbose=1, callbacks=[es], class_weight={0: 10, 1: 1})\n",
    "\n",
    "        model.save('model/result/'+model_name+'_'+str(fold))\n",
    "        with open('model/result/'+model_name+'_'+str(fold)+'.json', 'w') as fp:\n",
    "            json.dump(model.history.history, fp)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "38a5e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=[]\n",
    "fp=[]\n",
    "tn=[]\n",
    "fn=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "auc=[]\n",
    "prc=[]\n",
    "fold = []\n",
    "model = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    m='model' + str(i)\n",
    "    #print(m)\n",
    "    for j in range(1,6):\n",
    "        #print(j)\n",
    "        f = open('model/result/'+ m +'_'+str(j)+'.json')\n",
    "        result = json.load(f)\n",
    "        f.close()\n",
    "        max_prc = max(result['val_prc'])\n",
    "        index_max = result['val_prc'].index(max_prc)\n",
    "        tp.append(result['val_tp'][index_max])\n",
    "        fp.append(result['val_fp'][index_max])\n",
    "        tn.append(result['val_tn'][index_max])\n",
    "        fn.append(result['val_fp'][index_max])\n",
    "        precision.append(result['val_precision'][index_max])\n",
    "        recall.append(result['val_recall'][index_max])\n",
    "        auc.append(result['val_auc'][index_max])\n",
    "        prc.append(max_prc)\n",
    "        fold.append(j)\n",
    "        model.append(m)\n",
    "\n",
    "result=pd.DataFrame({\"tp\":tp,\n",
    "                     \"fp\":fp,\n",
    "                     \"tn\":tn,\n",
    "                     \"fn\":fn,\n",
    "                     \"precision\":precision,\n",
    "                     \"recall\":recall,\n",
    "                     \"auc\":auc,\n",
    "                     \"prc\":prc,\n",
    "                     \"fold\":fold,\n",
    "                     \"model\":model\n",
    "                    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d11d3285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>7601.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.949154</td>\n",
       "      <td>0.434607</td>\n",
       "      <td>1</td>\n",
       "      <td>model1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>178.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>12589.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.390351</td>\n",
       "      <td>0.419811</td>\n",
       "      <td>0.888406</td>\n",
       "      <td>0.439929</td>\n",
       "      <td>2</td>\n",
       "      <td>model1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>6789.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.227891</td>\n",
       "      <td>0.661308</td>\n",
       "      <td>0.382783</td>\n",
       "      <td>3</td>\n",
       "      <td>model1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144.0</td>\n",
       "      <td>4376.0</td>\n",
       "      <td>36052.0</td>\n",
       "      <td>4376.0</td>\n",
       "      <td>0.031858</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.057215</td>\n",
       "      <td>4</td>\n",
       "      <td>model1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>184.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>26452.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>0.016924</td>\n",
       "      <td>0.394850</td>\n",
       "      <td>0.552928</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>5</td>\n",
       "      <td>model1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>7638.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.851180</td>\n",
       "      <td>0.320973</td>\n",
       "      <td>1</td>\n",
       "      <td>model2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>61.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12823.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.143868</td>\n",
       "      <td>0.850701</td>\n",
       "      <td>0.298174</td>\n",
       "      <td>2</td>\n",
       "      <td>model2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>64.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6797.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.686723</td>\n",
       "      <td>0.281093</td>\n",
       "      <td>3</td>\n",
       "      <td>model2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>138.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>40005.0</td>\n",
       "      <td>423.0</td>\n",
       "      <td>0.245989</td>\n",
       "      <td>0.482517</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.386172</td>\n",
       "      <td>4</td>\n",
       "      <td>model2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>113.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>34876.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.047539</td>\n",
       "      <td>0.242489</td>\n",
       "      <td>0.658410</td>\n",
       "      <td>0.055895</td>\n",
       "      <td>5</td>\n",
       "      <td>model2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7669.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.662228</td>\n",
       "      <td>0.303358</td>\n",
       "      <td>1</td>\n",
       "      <td>model3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>12821.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.678773</td>\n",
       "      <td>0.206530</td>\n",
       "      <td>2</td>\n",
       "      <td>model3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>57.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>6796.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.636597</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>3</td>\n",
       "      <td>model3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>128.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>39923.0</td>\n",
       "      <td>505.0</td>\n",
       "      <td>0.202212</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.888736</td>\n",
       "      <td>0.299596</td>\n",
       "      <td>4</td>\n",
       "      <td>model3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>42.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>35811.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>0.030635</td>\n",
       "      <td>0.090129</td>\n",
       "      <td>0.512331</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>5</td>\n",
       "      <td>model3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>21.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>7359.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0.063253</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.822278</td>\n",
       "      <td>0.279830</td>\n",
       "      <td>1</td>\n",
       "      <td>model4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>117.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>12610.0</td>\n",
       "      <td>257.0</td>\n",
       "      <td>0.312834</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.804883</td>\n",
       "      <td>0.242010</td>\n",
       "      <td>2</td>\n",
       "      <td>model4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>90.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6795.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.574236</td>\n",
       "      <td>0.347924</td>\n",
       "      <td>3</td>\n",
       "      <td>model4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>129.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>39603.0</td>\n",
       "      <td>825.0</td>\n",
       "      <td>0.135220</td>\n",
       "      <td>0.451049</td>\n",
       "      <td>0.789040</td>\n",
       "      <td>0.204796</td>\n",
       "      <td>4</td>\n",
       "      <td>model4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>119.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>35107.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>0.055297</td>\n",
       "      <td>0.255365</td>\n",
       "      <td>0.573782</td>\n",
       "      <td>0.058037</td>\n",
       "      <td>5</td>\n",
       "      <td>model4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7669.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>1</td>\n",
       "      <td>model5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>79.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12847.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.186321</td>\n",
       "      <td>0.727240</td>\n",
       "      <td>0.345711</td>\n",
       "      <td>2</td>\n",
       "      <td>model5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>89.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6798.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.151361</td>\n",
       "      <td>0.776106</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>3</td>\n",
       "      <td>model5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>115.0</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>39179.0</td>\n",
       "      <td>1249.0</td>\n",
       "      <td>0.084311</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.898124</td>\n",
       "      <td>0.216421</td>\n",
       "      <td>4</td>\n",
       "      <td>model5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>128.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>32413.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.274678</td>\n",
       "      <td>0.619684</td>\n",
       "      <td>0.140856</td>\n",
       "      <td>5</td>\n",
       "      <td>model5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>22.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7648.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.843771</td>\n",
       "      <td>0.367120</td>\n",
       "      <td>1</td>\n",
       "      <td>model6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>88.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12775.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.207547</td>\n",
       "      <td>0.748075</td>\n",
       "      <td>0.285187</td>\n",
       "      <td>2</td>\n",
       "      <td>model6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>51.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6797.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>0.756629</td>\n",
       "      <td>0.447035</td>\n",
       "      <td>3</td>\n",
       "      <td>model6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>172.0</td>\n",
       "      <td>982.0</td>\n",
       "      <td>39446.0</td>\n",
       "      <td>982.0</td>\n",
       "      <td>0.149047</td>\n",
       "      <td>0.601399</td>\n",
       "      <td>0.875541</td>\n",
       "      <td>0.495487</td>\n",
       "      <td>4</td>\n",
       "      <td>model6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>114.0</td>\n",
       "      <td>924.0</td>\n",
       "      <td>36216.0</td>\n",
       "      <td>924.0</td>\n",
       "      <td>0.109827</td>\n",
       "      <td>0.244635</td>\n",
       "      <td>0.649890</td>\n",
       "      <td>0.206422</td>\n",
       "      <td>5</td>\n",
       "      <td>model6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>7503.0</td>\n",
       "      <td>167.0</td>\n",
       "      <td>0.208531</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.854740</td>\n",
       "      <td>0.639115</td>\n",
       "      <td>1</td>\n",
       "      <td>model7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>126.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>12575.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.301435</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.666869</td>\n",
       "      <td>0.309086</td>\n",
       "      <td>2</td>\n",
       "      <td>model7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>132.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>6792.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0.874172</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.769823</td>\n",
       "      <td>0.474813</td>\n",
       "      <td>3</td>\n",
       "      <td>model7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>108.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>35876.0</td>\n",
       "      <td>4552.0</td>\n",
       "      <td>0.023176</td>\n",
       "      <td>0.377622</td>\n",
       "      <td>0.752071</td>\n",
       "      <td>0.076935</td>\n",
       "      <td>4</td>\n",
       "      <td>model7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>137.0</td>\n",
       "      <td>4496.0</td>\n",
       "      <td>32644.0</td>\n",
       "      <td>4496.0</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.293991</td>\n",
       "      <td>0.609140</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>5</td>\n",
       "      <td>model7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>40.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>7475.0</td>\n",
       "      <td>195.0</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.606061</td>\n",
       "      <td>0.830180</td>\n",
       "      <td>0.393049</td>\n",
       "      <td>1</td>\n",
       "      <td>model8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>253.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>12510.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>0.414754</td>\n",
       "      <td>0.596698</td>\n",
       "      <td>0.928848</td>\n",
       "      <td>0.486710</td>\n",
       "      <td>2</td>\n",
       "      <td>model8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>128.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>6787.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.217687</td>\n",
       "      <td>0.665948</td>\n",
       "      <td>0.391623</td>\n",
       "      <td>3</td>\n",
       "      <td>model8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>115.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>36650.0</td>\n",
       "      <td>3778.0</td>\n",
       "      <td>0.029540</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.746887</td>\n",
       "      <td>0.104235</td>\n",
       "      <td>4</td>\n",
       "      <td>model8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>146.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>35117.0</td>\n",
       "      <td>2023.0</td>\n",
       "      <td>0.067312</td>\n",
       "      <td>0.313305</td>\n",
       "      <td>0.657316</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>5</td>\n",
       "      <td>model8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>45.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>7589.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.897633</td>\n",
       "      <td>0.631702</td>\n",
       "      <td>1</td>\n",
       "      <td>model9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>248.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>12341.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>0.320413</td>\n",
       "      <td>0.584906</td>\n",
       "      <td>0.913306</td>\n",
       "      <td>0.468049</td>\n",
       "      <td>2</td>\n",
       "      <td>model9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>109.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6797.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.886179</td>\n",
       "      <td>0.185374</td>\n",
       "      <td>0.760614</td>\n",
       "      <td>0.380237</td>\n",
       "      <td>3</td>\n",
       "      <td>model9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>126.0</td>\n",
       "      <td>3286.0</td>\n",
       "      <td>37142.0</td>\n",
       "      <td>3286.0</td>\n",
       "      <td>0.036928</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.824784</td>\n",
       "      <td>0.038613</td>\n",
       "      <td>4</td>\n",
       "      <td>model9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>191.0</td>\n",
       "      <td>7019.0</td>\n",
       "      <td>30121.0</td>\n",
       "      <td>7019.0</td>\n",
       "      <td>0.026491</td>\n",
       "      <td>0.409871</td>\n",
       "      <td>0.590323</td>\n",
       "      <td>0.082395</td>\n",
       "      <td>5</td>\n",
       "      <td>model9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>47.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>7352.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>0.712121</td>\n",
       "      <td>0.889377</td>\n",
       "      <td>0.610396</td>\n",
       "      <td>1</td>\n",
       "      <td>model10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>167.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>12594.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>0.379545</td>\n",
       "      <td>0.393868</td>\n",
       "      <td>0.849951</td>\n",
       "      <td>0.363167</td>\n",
       "      <td>2</td>\n",
       "      <td>model10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>132.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>6763.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.817457</td>\n",
       "      <td>0.442400</td>\n",
       "      <td>3</td>\n",
       "      <td>model10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>124.0</td>\n",
       "      <td>5557.0</td>\n",
       "      <td>34871.0</td>\n",
       "      <td>5557.0</td>\n",
       "      <td>0.021827</td>\n",
       "      <td>0.433566</td>\n",
       "      <td>0.739090</td>\n",
       "      <td>0.079873</td>\n",
       "      <td>4</td>\n",
       "      <td>model10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>186.0</td>\n",
       "      <td>9338.0</td>\n",
       "      <td>27802.0</td>\n",
       "      <td>9338.0</td>\n",
       "      <td>0.019530</td>\n",
       "      <td>0.399142</td>\n",
       "      <td>0.619807</td>\n",
       "      <td>0.070880</td>\n",
       "      <td>5</td>\n",
       "      <td>model10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tp       fp       tn       fn  precision    recall       auc       prc  \\\n",
       "0    30.0     69.0   7601.0     69.0   0.303030  0.454545  0.949154  0.434607   \n",
       "1   178.0    278.0  12589.0    278.0   0.390351  0.419811  0.888406  0.439929   \n",
       "2   134.0     22.0   6789.0     22.0   0.858974  0.227891  0.661308  0.382783   \n",
       "3   144.0   4376.0  36052.0   4376.0   0.031858  0.503497  0.805340  0.057215   \n",
       "4   184.0  10688.0  26452.0  10688.0   0.016924  0.394850  0.552928  0.027669   \n",
       "5    20.0     32.0   7638.0     32.0   0.384615  0.303030  0.851180  0.320973   \n",
       "6    61.0     44.0  12823.0     44.0   0.580952  0.143868  0.850701  0.298174   \n",
       "7    64.0     14.0   6797.0     14.0   0.820513  0.108844  0.686723  0.281093   \n",
       "8   138.0    423.0  40005.0    423.0   0.245989  0.482517  0.886486  0.386172   \n",
       "9   113.0   2264.0  34876.0   2264.0   0.047539  0.242489  0.658410  0.055895   \n",
       "10   18.0      1.0   7669.0      1.0   0.947368  0.272727  0.662228  0.303358   \n",
       "11   52.0     46.0  12821.0     46.0   0.530612  0.122642  0.678773  0.206530   \n",
       "12   57.0     15.0   6796.0     15.0   0.791667  0.096939  0.636597  0.310905   \n",
       "13  128.0    505.0  39923.0    505.0   0.202212  0.447552  0.888736  0.299596   \n",
       "14   42.0   1329.0  35811.0   1329.0   0.030635  0.090129  0.512331  0.024453   \n",
       "15   21.0    311.0   7359.0    311.0   0.063253  0.318182  0.822278  0.279830   \n",
       "16  117.0    257.0  12610.0    257.0   0.312834  0.275943  0.804883  0.242010   \n",
       "17   90.0     16.0   6795.0     16.0   0.849057  0.153061  0.574236  0.347924   \n",
       "18  129.0    825.0  39603.0    825.0   0.135220  0.451049  0.789040  0.204796   \n",
       "19  119.0   2033.0  35107.0   2033.0   0.055297  0.255365  0.573782  0.058037   \n",
       "20   20.0      1.0   7669.0      1.0   0.952381  0.303030  0.913924  0.435514   \n",
       "21   79.0     20.0  12847.0     20.0   0.797980  0.186321  0.727240  0.345711   \n",
       "22   89.0     13.0   6798.0     13.0   0.872549  0.151361  0.776106  0.427664   \n",
       "23  115.0   1249.0  39179.0   1249.0   0.084311  0.402098  0.898124  0.216421   \n",
       "24  128.0   4727.0  32413.0   4727.0   0.026365  0.274678  0.619684  0.140856   \n",
       "25   22.0     22.0   7648.0     22.0   0.500000  0.333333  0.843771  0.367120   \n",
       "26   88.0     92.0  12775.0     92.0   0.488889  0.207547  0.748075  0.285187   \n",
       "27   51.0     14.0   6797.0     14.0   0.784615  0.086735  0.756629  0.447035   \n",
       "28  172.0    982.0  39446.0    982.0   0.149047  0.601399  0.875541  0.495487   \n",
       "29  114.0    924.0  36216.0    924.0   0.109827  0.244635  0.649890  0.206422   \n",
       "30   44.0    167.0   7503.0    167.0   0.208531  0.666667  0.854740  0.639115   \n",
       "31  126.0    292.0  12575.0    292.0   0.301435  0.297170  0.666869  0.309086   \n",
       "32  132.0     19.0   6792.0     19.0   0.874172  0.224490  0.769823  0.474813   \n",
       "33  108.0   4552.0  35876.0   4552.0   0.023176  0.377622  0.752071  0.076935   \n",
       "34  137.0   4496.0  32644.0   4496.0   0.029570  0.293991  0.609140  0.057600   \n",
       "35   40.0    195.0   7475.0    195.0   0.170213  0.606061  0.830180  0.393049   \n",
       "36  253.0    357.0  12510.0    357.0   0.414754  0.596698  0.928848  0.486710   \n",
       "37  128.0     24.0   6787.0     24.0   0.842105  0.217687  0.665948  0.391623   \n",
       "38  115.0   3778.0  36650.0   3778.0   0.029540  0.402098  0.746887  0.104235   \n",
       "39  146.0   2023.0  35117.0   2023.0   0.067312  0.313305  0.657316  0.140214   \n",
       "40   45.0     81.0   7589.0     81.0   0.357143  0.681818  0.897633  0.631702   \n",
       "41  248.0    526.0  12341.0    526.0   0.320413  0.584906  0.913306  0.468049   \n",
       "42  109.0     14.0   6797.0     14.0   0.886179  0.185374  0.760614  0.380237   \n",
       "43  126.0   3286.0  37142.0   3286.0   0.036928  0.440559  0.824784  0.038613   \n",
       "44  191.0   7019.0  30121.0   7019.0   0.026491  0.409871  0.590323  0.082395   \n",
       "45   47.0    318.0   7352.0    318.0   0.128767  0.712121  0.889377  0.610396   \n",
       "46  167.0    273.0  12594.0    273.0   0.379545  0.393868  0.849951  0.363167   \n",
       "47  132.0     48.0   6763.0     48.0   0.733333  0.224490  0.817457  0.442400   \n",
       "48  124.0   5557.0  34871.0   5557.0   0.021827  0.433566  0.739090  0.079873   \n",
       "49  186.0   9338.0  27802.0   9338.0   0.019530  0.399142  0.619807  0.070880   \n",
       "\n",
       "    fold    model  \n",
       "0      1   model1  \n",
       "1      2   model1  \n",
       "2      3   model1  \n",
       "3      4   model1  \n",
       "4      5   model1  \n",
       "5      1   model2  \n",
       "6      2   model2  \n",
       "7      3   model2  \n",
       "8      4   model2  \n",
       "9      5   model2  \n",
       "10     1   model3  \n",
       "11     2   model3  \n",
       "12     3   model3  \n",
       "13     4   model3  \n",
       "14     5   model3  \n",
       "15     1   model4  \n",
       "16     2   model4  \n",
       "17     3   model4  \n",
       "18     4   model4  \n",
       "19     5   model4  \n",
       "20     1   model5  \n",
       "21     2   model5  \n",
       "22     3   model5  \n",
       "23     4   model5  \n",
       "24     5   model5  \n",
       "25     1   model6  \n",
       "26     2   model6  \n",
       "27     3   model6  \n",
       "28     4   model6  \n",
       "29     5   model6  \n",
       "30     1   model7  \n",
       "31     2   model7  \n",
       "32     3   model7  \n",
       "33     4   model7  \n",
       "34     5   model7  \n",
       "35     1   model8  \n",
       "36     2   model8  \n",
       "37     3   model8  \n",
       "38     4   model8  \n",
       "39     5   model8  \n",
       "40     1   model9  \n",
       "41     2   model9  \n",
       "42     3   model9  \n",
       "43     4   model9  \n",
       "44     5   model9  \n",
       "45     1  model10  \n",
       "46     2  model10  \n",
       "47     3  model10  \n",
       "48     4  model10  \n",
       "49     5  model10  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1c371c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>134.0</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>17896.6</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>0.320228</td>\n",
       "      <td>0.400119</td>\n",
       "      <td>0.771427</td>\n",
       "      <td>0.268441</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model10</th>\n",
       "      <td>131.2</td>\n",
       "      <td>3106.8</td>\n",
       "      <td>17876.4</td>\n",
       "      <td>3106.8</td>\n",
       "      <td>0.256601</td>\n",
       "      <td>0.432637</td>\n",
       "      <td>0.783136</td>\n",
       "      <td>0.313343</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>79.2</td>\n",
       "      <td>555.4</td>\n",
       "      <td>20427.8</td>\n",
       "      <td>555.4</td>\n",
       "      <td>0.415922</td>\n",
       "      <td>0.256150</td>\n",
       "      <td>0.786700</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>59.4</td>\n",
       "      <td>379.2</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>379.2</td>\n",
       "      <td>0.500499</td>\n",
       "      <td>0.205998</td>\n",
       "      <td>0.675733</td>\n",
       "      <td>0.228969</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>95.2</td>\n",
       "      <td>688.4</td>\n",
       "      <td>20294.8</td>\n",
       "      <td>688.4</td>\n",
       "      <td>0.283132</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.712844</td>\n",
       "      <td>0.226519</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>86.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>19781.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>0.263498</td>\n",
       "      <td>0.787015</td>\n",
       "      <td>0.313233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model6</th>\n",
       "      <td>89.4</td>\n",
       "      <td>406.8</td>\n",
       "      <td>20576.4</td>\n",
       "      <td>406.8</td>\n",
       "      <td>0.406476</td>\n",
       "      <td>0.294730</td>\n",
       "      <td>0.774781</td>\n",
       "      <td>0.360250</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model7</th>\n",
       "      <td>109.4</td>\n",
       "      <td>1905.2</td>\n",
       "      <td>19078.0</td>\n",
       "      <td>1905.2</td>\n",
       "      <td>0.287377</td>\n",
       "      <td>0.371988</td>\n",
       "      <td>0.730528</td>\n",
       "      <td>0.311510</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model8</th>\n",
       "      <td>136.4</td>\n",
       "      <td>1275.4</td>\n",
       "      <td>19707.8</td>\n",
       "      <td>1275.4</td>\n",
       "      <td>0.304785</td>\n",
       "      <td>0.427170</td>\n",
       "      <td>0.765836</td>\n",
       "      <td>0.303166</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model9</th>\n",
       "      <td>143.8</td>\n",
       "      <td>2185.2</td>\n",
       "      <td>18798.0</td>\n",
       "      <td>2185.2</td>\n",
       "      <td>0.325431</td>\n",
       "      <td>0.460506</td>\n",
       "      <td>0.797332</td>\n",
       "      <td>0.320199</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tp      fp       tn      fn  precision    recall       auc  \\\n",
       "model                                                                    \n",
       "model1   134.0  3086.6  17896.6  3086.6   0.320228  0.400119  0.771427   \n",
       "model10  131.2  3106.8  17876.4  3106.8   0.256601  0.432637  0.783136   \n",
       "model2    79.2   555.4  20427.8   555.4   0.415922  0.256150  0.786700   \n",
       "model3    59.4   379.2  20604.0   379.2   0.500499  0.205998  0.675733   \n",
       "model4    95.2   688.4  20294.8   688.4   0.283132  0.290720  0.712844   \n",
       "model5    86.2  1202.0  19781.2  1202.0   0.546717  0.263498  0.787015   \n",
       "model6    89.4   406.8  20576.4   406.8   0.406476  0.294730  0.774781   \n",
       "model7   109.4  1905.2  19078.0  1905.2   0.287377  0.371988  0.730528   \n",
       "model8   136.4  1275.4  19707.8  1275.4   0.304785  0.427170  0.765836   \n",
       "model9   143.8  2185.2  18798.0  2185.2   0.325431  0.460506  0.797332   \n",
       "\n",
       "              prc  fold  \n",
       "model                    \n",
       "model1   0.268441     3  \n",
       "model10  0.313343     3  \n",
       "model2   0.268462     3  \n",
       "model3   0.228969     3  \n",
       "model4   0.226519     3  \n",
       "model5   0.313233     3  \n",
       "model6   0.360250     3  \n",
       "model7   0.311510     3  \n",
       "model8   0.303166     3  \n",
       "model9   0.320199     3  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4f05fdc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>12589.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.419811</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.382783</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model10</th>\n",
       "      <td>132.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>12594.0</td>\n",
       "      <td>318.0</td>\n",
       "      <td>0.128767</td>\n",
       "      <td>0.399142</td>\n",
       "      <td>0.817457</td>\n",
       "      <td>0.363167</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>64.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12823.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.242489</td>\n",
       "      <td>0.850701</td>\n",
       "      <td>0.298174</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>12821.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.662228</td>\n",
       "      <td>0.299596</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>117.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>12610.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0.135220</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.789040</td>\n",
       "      <td>0.242010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>89.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12847.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.274678</td>\n",
       "      <td>0.776106</td>\n",
       "      <td>0.345711</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model6</th>\n",
       "      <td>88.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>12775.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.244635</td>\n",
       "      <td>0.756629</td>\n",
       "      <td>0.367120</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model7</th>\n",
       "      <td>126.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>12575.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>0.208531</td>\n",
       "      <td>0.297170</td>\n",
       "      <td>0.752071</td>\n",
       "      <td>0.309086</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model8</th>\n",
       "      <td>128.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>12510.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>0.170213</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.746887</td>\n",
       "      <td>0.391623</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model9</th>\n",
       "      <td>126.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>12341.0</td>\n",
       "      <td>526.0</td>\n",
       "      <td>0.320413</td>\n",
       "      <td>0.440559</td>\n",
       "      <td>0.824784</td>\n",
       "      <td>0.380237</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            tp     fp       tn     fn  precision    recall       auc  \\\n",
       "model                                                                  \n",
       "model1   144.0  278.0  12589.0  278.0   0.303030  0.419811  0.805340   \n",
       "model10  132.0  318.0  12594.0  318.0   0.128767  0.399142  0.817457   \n",
       "model2    64.0   44.0  12823.0   44.0   0.384615  0.242489  0.850701   \n",
       "model3    52.0   46.0  12821.0   46.0   0.530612  0.122642  0.662228   \n",
       "model4   117.0  311.0  12610.0  311.0   0.135220  0.275943  0.789040   \n",
       "model5    89.0   20.0  12847.0   20.0   0.797980  0.274678  0.776106   \n",
       "model6    88.0   92.0  12775.0   92.0   0.488889  0.244635  0.756629   \n",
       "model7   126.0  292.0  12575.0  292.0   0.208531  0.297170  0.752071   \n",
       "model8   128.0  357.0  12510.0  357.0   0.170213  0.402098  0.746887   \n",
       "model9   126.0  526.0  12341.0  526.0   0.320413  0.440559  0.824784   \n",
       "\n",
       "              prc  fold  \n",
       "model                    \n",
       "model1   0.382783     3  \n",
       "model10  0.363167     3  \n",
       "model2   0.298174     3  \n",
       "model3   0.299596     3  \n",
       "model4   0.242010     3  \n",
       "model5   0.345711     3  \n",
       "model6   0.367120     3  \n",
       "model7   0.309086     3  \n",
       "model8   0.391623     3  \n",
       "model9   0.380237     3  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80912e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>61.951594</td>\n",
       "      <td>4632.217374</td>\n",
       "      <td>12848.756683</td>\n",
       "      <td>4632.217374</td>\n",
       "      <td>0.343005</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.163010</td>\n",
       "      <td>0.207775</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>46.547825</td>\n",
       "      <td>970.236466</td>\n",
       "      <td>15805.283284</td>\n",
       "      <td>970.236466</td>\n",
       "      <td>0.298535</td>\n",
       "      <td>0.148268</td>\n",
       "      <td>0.105670</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>41.180092</td>\n",
       "      <td>571.105244</td>\n",
       "      <td>15992.394974</td>\n",
       "      <td>571.105244</td>\n",
       "      <td>0.385722</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.135825</td>\n",
       "      <td>0.122030</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>43.922659</td>\n",
       "      <td>807.351720</td>\n",
       "      <td>15818.055576</td>\n",
       "      <td>807.351720</td>\n",
       "      <td>0.332899</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.127282</td>\n",
       "      <td>0.108027</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>41.889139</td>\n",
       "      <td>2042.123894</td>\n",
       "      <td>14993.240284</td>\n",
       "      <td>2042.123894</td>\n",
       "      <td>0.452341</td>\n",
       "      <td>0.099284</td>\n",
       "      <td>0.122620</td>\n",
       "      <td>0.130564</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model6</th>\n",
       "      <td>57.981031</td>\n",
       "      <td>499.953198</td>\n",
       "      <td>15957.256384</td>\n",
       "      <td>499.953198</td>\n",
       "      <td>0.279642</td>\n",
       "      <td>0.192943</td>\n",
       "      <td>0.088815</td>\n",
       "      <td>0.117437</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model7</th>\n",
       "      <td>38.168049</td>\n",
       "      <td>2392.660590</td>\n",
       "      <td>14083.896744</td>\n",
       "      <td>2392.660590</td>\n",
       "      <td>0.348930</td>\n",
       "      <td>0.173424</td>\n",
       "      <td>0.095163</td>\n",
       "      <td>0.251741</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tp           fp            tn           fn  precision  \\\n",
       "model                                                                  \n",
       "model1  61.951594  4632.217374  12848.756683  4632.217374   0.343005   \n",
       "model2  46.547825   970.236466  15805.283284   970.236466   0.298535   \n",
       "model3  41.180092   571.105244  15992.394974   571.105244   0.385722   \n",
       "model4  43.922659   807.351720  15818.055576   807.351720   0.332899   \n",
       "model5  41.889139  2042.123894  14993.240284  2042.123894   0.452341   \n",
       "model6  57.981031   499.953198  15957.256384   499.953198   0.279642   \n",
       "model7  38.168049  2392.660590  14083.896744  2392.660590   0.348930   \n",
       "\n",
       "          recall       auc       prc      fold  \n",
       "model                                           \n",
       "model1  0.104555  0.163010  0.207775  1.581139  \n",
       "model2  0.148268  0.105670  0.125340  1.581139  \n",
       "model3  0.154167  0.135825  0.122030  1.581139  \n",
       "model4  0.108256  0.127282  0.108027  1.581139  \n",
       "model5  0.099284  0.122620  0.130564  1.581139  \n",
       "model6  0.192943  0.088815  0.117437  1.581139  \n",
       "model7  0.173424  0.095163  0.251741  1.581139  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0d4d42f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>184.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>36052.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.949154</td>\n",
       "      <td>0.439929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>138.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>40005.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.482517</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.386172</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>128.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>39923.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.888736</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>129.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>39603.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.451049</td>\n",
       "      <td>0.822278</td>\n",
       "      <td>0.347924</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>128.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>39179.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tp       fp       tn       fn  precision    recall       auc  \\\n",
       "model                                                                     \n",
       "model1  184.0  10688.0  36052.0  10688.0   0.858974  0.503497  0.949154   \n",
       "model2  138.0   2264.0  40005.0   2264.0   0.820513  0.482517  0.886486   \n",
       "model3  128.0   1329.0  39923.0   1329.0   0.947368  0.447552  0.888736   \n",
       "model4  129.0   2033.0  39603.0   2033.0   0.849057  0.451049  0.822278   \n",
       "model5  128.0   4727.0  39179.0   4727.0   0.952381  0.402098  0.913924   \n",
       "\n",
       "             prc  fold  \n",
       "model                   \n",
       "model1  0.439929     5  \n",
       "model2  0.386172     5  \n",
       "model3  0.310905     5  \n",
       "model4  0.347924     5  \n",
       "model5  0.435514     5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77113f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    #training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "    #                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = word_embedding_layer(inputs)\n",
    "    outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=256, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/embedding16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/embedding16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e7158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb5b93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'model/transformer16_fold'+str(fold)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d4c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    #outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307c3e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4cf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "fold=1\n",
    "data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "data_train=data_all[data_all['set']=='train']\n",
    "data_valid=data_all[data_all['set']=='valid']\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train=encode_padding(data_train, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_train=data_train['label']\n",
    "X_valid=encode_padding(data_valid, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_valid=data_valid['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 4  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 128  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df027f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6705508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2ee6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfc4806",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5ee6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "#outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = word_embedding_layer(inputs)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4f6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = K.max(outputs,axis=-1)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9a5324",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = GroupShuffleSplit(test_size=.20, n_splits=2)\n",
    "\n",
    "while True:\n",
    "    split = valid_split.split(data, y, groups=g)\n",
    "    train_inds, valid_inds = next(split)\n",
    "    if len(train_inds)/len(valid_inds)>3:\n",
    "        break\n",
    "    \n",
    "data_train=data.iloc[train_inds,:]\n",
    "data_valid=data.iloc[valid_inds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0b8e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ff5be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fa82a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos=data_train[data_train.label==1]\n",
    "data_train_pos_new = data_train_pos.sample(n=data_train.shape[0], random_state=1, replace=True)\n",
    "\n",
    "for i in range(data_train_pos_new.shape[0]):\n",
    "    r=random.uniform(0, 1)\n",
    "    if r>0.75:\n",
    "        tmp=random.sample([-1, -2, -3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][:tmp[0]]\n",
    "    if r >0.5 and r <=0.75:\n",
    "        tmp=random.sample([1, 2, 3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][tmp[0]:]\n",
    "    if r >0.25 and r <=0.5:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i] + tmp\n",
    "    else:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=tmp + data_train_pos_new.seq.iloc[i]\n",
    "\n",
    "data_train_all = pd.concat([data_train, data_train_pos_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb0b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all = data_train_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82de513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all['set']='train'\n",
    "data_valid['set']='valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867c13fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21194d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train_all, data_valid])\n",
    "data_all.reset_index(drop=True)\n",
    "data_all['id']=str(fold)+'_'+data_all.index.astype(str)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c630c041",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('data/fold'+str(fold)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ceb48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=one_hot_encode_padding(data_train_all, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_train=data_train_all['label']\n",
    "X_valid=one_hot_encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_valid=data_valid['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3052ab33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0cd494",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(y_valid)\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(y_valid)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d7a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = train_model(X_train, y_train, (X_valid, y_valid), channel=CFG.channel,\n",
    "                   nb_epoch=10, border_mode='same',\n",
    "                   inp_len=CFG.feature_seq_len, nodes=40, layers=5, nbr_filters=120, filter_len=8, dropout1=0,\n",
    "                   dropout2=0, dropout3=0.2, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ab12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a6fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/IRESbase_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db41dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/RF00031.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d59222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=one_hot_encode_padding(IRESbase, col='IRES.Sequence', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c11149",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636f7d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008aea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ad2118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1d53af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296c758",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b95c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 8  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 8  # Dimensionality of the linearly projected values\n",
    "d_ff = 8  # Dimensionality of the inner fully connected layer\n",
    "d_model = 8  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c35b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361a12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fac6c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_padding(df, col='utr', seq_len=50, padding='5end', channel=1):\n",
    "    # 5end padding means pad the left end (5' end) if sequence length < seq_len; keep the seq_len right end (3' end) if  sequence length > seq_len\n",
    "    # 3end padding means pad the right end (3' end) if sequence length < seq_len; keep the seq_len left end (5' end) if  sequence length > seq_len\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1],'c':[2],'g':[3],'t':[4], 'n':[0], '(':[5],')':[6],'.':[7]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.zeros([len(df),seq_len,channel])\n",
    "    \n",
    "    # Iterate through UTRs and one-hot encode\n",
    "    for i,seq in enumerate(df[col]):\n",
    "        if(isinstance(seq, str)):\n",
    "            if(padding=='3end'):\n",
    "                seq=seq[:min(len(seq),seq_len)]\n",
    "            if(padding=='5end'):\n",
    "                seq=seq[max(0,(len(seq)-seq_len)):len(seq)]\n",
    "            seq = seq.lower()\n",
    "            a = np.array([nuc_d[x] for x in seq])\n",
    "            if(padding=='5end'):\n",
    "                vectors[i, (seq_len-len(seq)):seq_len] = a\n",
    "            if(padding=='3end'):\n",
    "                vectors[i, :len(seq)] = a\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b734b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.squeeze(encode_padding(data_train_all, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_train=data_train_all['label']\n",
    "X_valid=np.squeeze(encode_padding(data_valid, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_valid=data_valid['label']\n",
    "#m_valid=np.squeeze(tf.math.equal(X_valid, 0))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea5247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aa830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3fdc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 64 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 64 # Vocabulary size for the decoder\n",
    " \n",
    "enc_seq_length = 200  # Maximum length of the input sequence\n",
    "dec_seq_length = 200  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 64  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31338",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8aa2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    " \n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    " \n",
    "    def __call__(self, step_num):\n",
    " \n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    " \n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d91c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "      keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "      keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "      keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "          batch_size=256, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dda480",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce74ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f793f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1722262",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(IRESbase['IRES.Sequence'].to_numpy(), \n",
    "                                              value='N', padding='post', truncating='post', maxlen=600, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f8528",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.TextVectorization('ATCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f82d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78dd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = [\"A\", \"C\", \"G\", \"T\"]\n",
    "max_len = CFG.feature_seq_len\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len,\n",
    " vocabulary=vocab_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3870575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t  = Tokenizer(num_words=5,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=False, char_level=True, oov_token=None,\n",
    "    document_count=0)\n",
    "fit_text = \"ACGTN\"\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "test_text = \"NCGTA\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499b383",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid['seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede7d99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=t.texts_to_sequences(data_valid['seq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f620fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        import numpy as np\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974c5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25250ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d126ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060dd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0496496",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cb7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70e7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "with open('nucl.json') as f:\n",
    "    contents = f.readlines()\n",
    "    t = tokenizer_from_json(contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0f207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba5096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b07a4092d17fabcc770aac63fd8b04ec6822179f9cd74920d0d99ada38e73abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
