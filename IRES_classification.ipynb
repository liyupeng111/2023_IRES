{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d39a341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, convert_to_tensor, string\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow import linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, MaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.layers import Activation, Flatten, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import softmax\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Parameters for plotting model results ###\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.labelpad'] = 5\n",
    "plt.rcParams['axes.linewidth']= 2\n",
    "plt.rcParams['xtick.labelsize']= 14\n",
    "plt.rcParams['ytick.labelsize']= 14\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.minor.size'] = 3\n",
    "plt.rcParams['ytick.minor.size'] = 3\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cedd5a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46d9ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "from functions import LRScheduler, TransformerModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65ca1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.3.4\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version: ' + tf. __version__)\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265c1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic random seed\n",
    "DEFAULT_RANDOM_SEED = 2022\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# tensorflow random seed \n",
    "def seedTF(seed=DEFAULT_RANDOM_SEED):\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "# import torch\n",
    "# def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + tensorflow + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTF(seed)\n",
    "    # seedTorch(seed)\n",
    "\n",
    "seedEverything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd3a43b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0c1e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model6\n",
      "fold1\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 600)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 600, 16)           80        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 600, 128)          16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 561,809\n",
      "Trainable params: 561,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5010/5010 [==============================] - 195s 39ms/step - loss: 0.0682 - tp: 142500.0000 - fp: 735.0000 - tn: 158171.0000 - fn: 19190.0000 - accuracy: 0.9379 - precision: 0.9949 - recall: 0.8813 - auc: 0.9947 - prc: 0.9951 - val_loss: 0.0250 - val_tp: 21.0000 - val_fp: 62.0000 - val_tn: 7608.0000 - val_fn: 45.0000 - val_accuracy: 0.9862 - val_precision: 0.2530 - val_recall: 0.3182 - val_auc: 0.6110 - val_prc: 0.2883\n",
      "Epoch 2/10\n",
      "5010/5010 [==============================] - 196s 39ms/step - loss: 0.0071 - tp: 160173.0000 - fp: 112.0000 - tn: 158794.0000 - fn: 1517.0000 - accuracy: 0.9949 - precision: 0.9993 - recall: 0.9906 - auc: 0.9999 - prc: 0.9999 - val_loss: 0.0205 - val_tp: 22.0000 - val_fp: 34.0000 - val_tn: 7636.0000 - val_fn: 44.0000 - val_accuracy: 0.9899 - val_precision: 0.3929 - val_recall: 0.3333 - val_auc: 0.7555 - val_prc: 0.3387\n",
      "Epoch 3/10\n",
      "5010/5010 [==============================] - 193s 39ms/step - loss: 0.0028 - tp: 161179.0000 - fp: 56.0000 - tn: 158850.0000 - fn: 511.0000 - accuracy: 0.9982 - precision: 0.9997 - recall: 0.9968 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0214 - val_tp: 22.0000 - val_fp: 8.0000 - val_tn: 7662.0000 - val_fn: 44.0000 - val_accuracy: 0.9933 - val_precision: 0.7333 - val_recall: 0.3333 - val_auc: 0.8321 - val_prc: 0.3576\n",
      "Epoch 4/10\n",
      "5010/5010 [==============================] - 197s 39ms/step - loss: 0.0018 - tp: 161416.0000 - fp: 35.0000 - tn: 158871.0000 - fn: 274.0000 - accuracy: 0.9990 - precision: 0.9998 - recall: 0.9983 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0250 - val_tp: 22.0000 - val_fp: 22.0000 - val_tn: 7648.0000 - val_fn: 44.0000 - val_accuracy: 0.9915 - val_precision: 0.5000 - val_recall: 0.3333 - val_auc: 0.8438 - val_prc: 0.3671\n",
      "Epoch 5/10\n",
      "5010/5010 [==============================] - 193s 39ms/step - loss: 0.0017 - tp: 161414.0000 - fp: 27.0000 - tn: 158879.0000 - fn: 276.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9983 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0233 - val_tp: 22.0000 - val_fp: 25.0000 - val_tn: 7645.0000 - val_fn: 44.0000 - val_accuracy: 0.9911 - val_precision: 0.4681 - val_recall: 0.3333 - val_auc: 0.8457 - val_prc: 0.3570\n",
      "Epoch 6/10\n",
      "5010/5010 [==============================] - 195s 39ms/step - loss: 9.3301e-04 - tp: 161533.0000 - fp: 13.0000 - tn: 158893.0000 - fn: 157.0000 - accuracy: 0.9995 - precision: 0.9999 - recall: 0.9990 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0343 - val_tp: 15.0000 - val_fp: 2.0000 - val_tn: 7668.0000 - val_fn: 51.0000 - val_accuracy: 0.9931 - val_precision: 0.8824 - val_recall: 0.2273 - val_auc: 0.6345 - val_prc: 0.2754\n",
      "Epoch 7/10\n",
      "5009/5010 [============================>.] - ETA: 0s - loss: 5.4300e-04 - tp: 161604.0000 - fp: 9.0000 - tn: 158885.0000 - fn: 78.0000 - accuracy: 0.9997 - precision: 0.9999 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "5010/5010 [==============================] - 194s 39ms/step - loss: 5.4297e-04 - tp: 161612.0000 - fp: 9.0000 - tn: 158897.0000 - fn: 78.0000 - accuracy: 0.9997 - precision: 0.9999 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0297 - val_tp: 20.0000 - val_fp: 12.0000 - val_tn: 7658.0000 - val_fn: 46.0000 - val_accuracy: 0.9925 - val_precision: 0.6250 - val_recall: 0.3030 - val_auc: 0.8408 - val_prc: 0.3652\n",
      "Epoch 00007: early stopping\n",
      "INFO:tensorflow:Assets written to: model/result/model6_1/assets\n",
      "fold2\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 600)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 600, 16)           80        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 600, 128)          16512     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 561,809\n",
      "Trainable params: 561,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4836/4836 [==============================] - 194s 40ms/step - loss: 0.0589 - tp: 139009.0000 - fp: 457.0000 - tn: 153252.0000 - fn: 16768.0000 - accuracy: 0.9443 - precision: 0.9967 - recall: 0.8924 - auc: 0.9959 - prc: 0.9961 - val_loss: 0.0787 - val_tp: 83.0000 - val_fp: 68.0000 - val_tn: 12799.0000 - val_fn: 341.0000 - val_accuracy: 0.9692 - val_precision: 0.5497 - val_recall: 0.1958 - val_auc: 0.6844 - val_prc: 0.2659\n",
      "Epoch 2/10\n",
      "4836/4836 [==============================] - 193s 40ms/step - loss: 0.0055 - tp: 154810.0000 - fp: 87.0000 - tn: 153622.0000 - fn: 967.0000 - accuracy: 0.9966 - precision: 0.9994 - recall: 0.9938 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1342 - val_tp: 75.0000 - val_fp: 26.0000 - val_tn: 12841.0000 - val_fn: 349.0000 - val_accuracy: 0.9718 - val_precision: 0.7426 - val_recall: 0.1769 - val_auc: 0.7125 - val_prc: 0.2613\n",
      "Epoch 3/10\n",
      "4836/4836 [==============================] - 192s 40ms/step - loss: 0.0030 - tp: 155339.0000 - fp: 46.0000 - tn: 153663.0000 - fn: 438.0000 - accuracy: 0.9984 - precision: 0.9997 - recall: 0.9972 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1037 - val_tp: 88.0000 - val_fp: 92.0000 - val_tn: 12775.0000 - val_fn: 336.0000 - val_accuracy: 0.9678 - val_precision: 0.4889 - val_recall: 0.2075 - val_auc: 0.7481 - val_prc: 0.2852\n",
      "Epoch 4/10\n",
      "4836/4836 [==============================] - 192s 40ms/step - loss: 0.0015 - tp: 155555.0000 - fp: 22.0000 - tn: 153687.0000 - fn: 222.0000 - accuracy: 0.9992 - precision: 0.9999 - recall: 0.9986 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1013 - val_tp: 84.0000 - val_fp: 54.0000 - val_tn: 12813.0000 - val_fn: 340.0000 - val_accuracy: 0.9704 - val_precision: 0.6087 - val_recall: 0.1981 - val_auc: 0.7291 - val_prc: 0.2684\n",
      "Epoch 5/10\n",
      "4681/4836 [============================>.] - ETA: 6s - loss: 7.4665e-04 - tp: 150648.0000 - fp: 13.0000 - tn: 148808.0000 - fn: 115.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9992 - auc: 1.0000 - prc: 1.0000"
     ]
    }
   ],
   "source": [
    "channels = [1, 4, 1, 1, 4, 1, 1, 1]\n",
    "\n",
    "for i in range(6,9):\n",
    "    model_name='model' + str(i)\n",
    "    print(model_name)\n",
    "\n",
    "    channel = channels[i-1]\n",
    "    if channel == 4:\n",
    "        padding_method = 'one_hot_encode_padding'\n",
    "    if channel ==1:\n",
    "        padding_method = 'encode_padding'\n",
    "\n",
    "    max_input_length = 600\n",
    "    dropout_rate = 0.1\n",
    "    feature_pad_end= '3end'\n",
    "\n",
    "    import models\n",
    "    custom_model = getattr(models, model_name)\n",
    "    import functions\n",
    "    custom_padding = getattr(functions, padding_method)\n",
    "    \n",
    "    for fold in range(1,6):\n",
    "        print('fold'+str(fold))\n",
    "        \n",
    "        data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "        data_train=data_all[data_all['set']=='train']\n",
    "        data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "        data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "        data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "        X_train=custom_padding(data_train, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "        y_train=data_train['label']\n",
    "        X_valid=custom_padding(data_valid, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "        y_valid=data_valid['label']\n",
    "    \n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        model=custom_model(seq_length=max_input_length, dropout_rate=dropout_rate)\n",
    "\n",
    "        #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "        adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "        METRICS = [\n",
    "              keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "              keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "              keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "              keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "              keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "              keras.metrics.Precision(name='precision'),\n",
    "              keras.metrics.Recall(name='recall'),\n",
    "              keras.metrics.AUC(name='auc'),\n",
    "              keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "        ]\n",
    "\n",
    "        model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "        model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "                  batch_size=64, epochs=10, verbose=1, callbacks=[es], class_weight={0: 10, 1: 1})\n",
    "\n",
    "        model.save('model/result/'+model_name+'_'+str(fold))\n",
    "        with open('model/result/'+model_name+'_'+str(fold)+'.json', 'w') as fp:\n",
    "            json.dump(model.history.history, fp)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc3d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='model5'\n",
    "\n",
    "channel = 4\n",
    "if channel == 4:\n",
    "    padding_method = 'one_hot_encode_padding'\n",
    "if channel ==1:\n",
    "    padding_method = 'encode_padding'\n",
    "\n",
    "max_input_length = 600\n",
    "dropout_rate = 0.1\n",
    "feature_pad_end= '3end'\n",
    "\n",
    "import models\n",
    "custom_model = getattr(models, model_name)\n",
    "import functions\n",
    "custom_padding = getattr(functions, padding_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6eaf000b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 600, 128)          4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 549,441\n",
      "Trainable params: 549,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5010/5010 [==============================] - 178s 35ms/step - loss: 0.0442 - tp: 149007.0000 - fp: 452.0000 - tn: 158454.0000 - fn: 12683.0000 - accuracy: 0.9590 - precision: 0.9970 - recall: 0.9216 - auc: 0.9979 - prc: 0.9980 - val_loss: 0.0153 - val_tp: 21.0000 - val_fp: 5.0000 - val_tn: 7665.0000 - val_fn: 45.0000 - val_accuracy: 0.9935 - val_precision: 0.8077 - val_recall: 0.3182 - val_auc: 0.8316 - val_prc: 0.3584\n",
      "Epoch 2/10\n",
      "5010/5010 [==============================] - 175s 35ms/step - loss: 0.0037 - tp: 160919.0000 - fp: 65.0000 - tn: 158841.0000 - fn: 771.0000 - accuracy: 0.9974 - precision: 0.9996 - recall: 0.9952 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0171 - val_tp: 21.0000 - val_fp: 18.0000 - val_tn: 7652.0000 - val_fn: 45.0000 - val_accuracy: 0.9919 - val_precision: 0.5385 - val_recall: 0.3182 - val_auc: 0.8527 - val_prc: 0.3398\n",
      "Epoch 3/10\n",
      "5010/5010 [==============================] - 178s 35ms/step - loss: 0.0014 - tp: 161425.0000 - fp: 29.0000 - tn: 158877.0000 - fn: 265.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9984 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0216 - val_tp: 22.0000 - val_fp: 5.0000 - val_tn: 7665.0000 - val_fn: 44.0000 - val_accuracy: 0.9937 - val_precision: 0.8148 - val_recall: 0.3333 - val_auc: 0.8304 - val_prc: 0.3654\n",
      "Epoch 4/10\n",
      "5010/5010 [==============================] - 177s 35ms/step - loss: 0.0015 - tp: 161473.0000 - fp: 24.0000 - tn: 158882.0000 - fn: 217.0000 - accuracy: 0.9992 - precision: 0.9999 - recall: 0.9987 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0155 - val_tp: 20.0000 - val_fp: 9.0000 - val_tn: 7661.0000 - val_fn: 46.0000 - val_accuracy: 0.9929 - val_precision: 0.6897 - val_recall: 0.3030 - val_auc: 0.8895 - val_prc: 0.3695\n",
      "Epoch 5/10\n",
      "5010/5010 [==============================] - 182s 36ms/step - loss: 6.6242e-04 - tp: 161576.0000 - fp: 9.0000 - tn: 158897.0000 - fn: 114.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0145 - val_tp: 23.0000 - val_fp: 25.0000 - val_tn: 7645.0000 - val_fn: 43.0000 - val_accuracy: 0.9912 - val_precision: 0.4792 - val_recall: 0.3485 - val_auc: 0.9011 - val_prc: 0.3744\n",
      "Epoch 6/10\n",
      "5010/5010 [==============================] - 185s 37ms/step - loss: 6.1371e-04 - tp: 161583.0000 - fp: 13.0000 - tn: 158893.0000 - fn: 107.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0225 - val_tp: 21.0000 - val_fp: 10.0000 - val_tn: 7660.0000 - val_fn: 45.0000 - val_accuracy: 0.9929 - val_precision: 0.6774 - val_recall: 0.3182 - val_auc: 0.8899 - val_prc: 0.3711\n",
      "Epoch 7/10\n",
      "5010/5010 [==============================] - 177s 35ms/step - loss: 5.9504e-04 - tp: 161605.0000 - fp: 7.0000 - tn: 158899.0000 - fn: 85.0000 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0204 - val_tp: 22.0000 - val_fp: 4.0000 - val_tn: 7666.0000 - val_fn: 44.0000 - val_accuracy: 0.9938 - val_precision: 0.8462 - val_recall: 0.3333 - val_auc: 0.8716 - val_prc: 0.4268\n",
      "Epoch 8/10\n",
      "5010/5010 [==============================] - 176s 35ms/step - loss: 6.4428e-04 - tp: 161605.0000 - fp: 8.0000 - tn: 158898.0000 - fn: 85.0000 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0212 - val_tp: 20.0000 - val_fp: 7.0000 - val_tn: 7663.0000 - val_fn: 46.0000 - val_accuracy: 0.9931 - val_precision: 0.7407 - val_recall: 0.3030 - val_auc: 0.8748 - val_prc: 0.4040\n",
      "Epoch 9/10\n",
      "5010/5010 [==============================] - 177s 35ms/step - loss: 6.4971e-04 - tp: 161602.0000 - fp: 12.0000 - tn: 158894.0000 - fn: 88.0000 - accuracy: 0.9997 - precision: 0.9999 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0199 - val_tp: 20.0000 - val_fp: 1.0000 - val_tn: 7669.0000 - val_fn: 46.0000 - val_accuracy: 0.9939 - val_precision: 0.9524 - val_recall: 0.3030 - val_auc: 0.9139 - val_prc: 0.4355\n",
      "Epoch 10/10\n",
      "5010/5010 [==============================] - 175s 35ms/step - loss: 5.1217e-04 - tp: 161613.0000 - fp: 7.0000 - tn: 158899.0000 - fn: 77.0000 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9995 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0215 - val_tp: 21.0000 - val_fp: 3.0000 - val_tn: 7667.0000 - val_fn: 45.0000 - val_accuracy: 0.9938 - val_precision: 0.8750 - val_recall: 0.3182 - val_auc: 0.8817 - val_prc: 0.4097\n",
      "INFO:tensorflow:Assets written to: model/result/model5_1/assets\n",
      "2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 600, 128)          4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 549,441\n",
      "Trainable params: 549,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4836/4836 [==============================] - 173s 36ms/step - loss: 0.0417 - tp: 144171.0000 - fp: 361.0000 - tn: 153348.0000 - fn: 11606.0000 - accuracy: 0.9613 - precision: 0.9975 - recall: 0.9255 - auc: 0.9980 - prc: 0.9981 - val_loss: 0.0776 - val_tp: 101.0000 - val_fp: 76.0000 - val_tn: 12791.0000 - val_fn: 323.0000 - val_accuracy: 0.9700 - val_precision: 0.5706 - val_recall: 0.2382 - val_auc: 0.6184 - val_prc: 0.2813\n",
      "Epoch 2/10\n",
      "4836/4836 [==============================] - 171s 35ms/step - loss: 0.0030 - tp: 155285.0000 - fp: 49.0000 - tn: 153660.0000 - fn: 492.0000 - accuracy: 0.9983 - precision: 0.9997 - recall: 0.9968 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1273 - val_tp: 79.0000 - val_fp: 20.0000 - val_tn: 12847.0000 - val_fn: 345.0000 - val_accuracy: 0.9725 - val_precision: 0.7980 - val_recall: 0.1863 - val_auc: 0.7272 - val_prc: 0.3457\n",
      "Epoch 3/10\n",
      "4836/4836 [==============================] - 190s 39ms/step - loss: 0.0015 - tp: 155528.0000 - fp: 28.0000 - tn: 153681.0000 - fn: 249.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9984 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0787 - val_tp: 104.0000 - val_fp: 124.0000 - val_tn: 12743.0000 - val_fn: 320.0000 - val_accuracy: 0.9666 - val_precision: 0.4561 - val_recall: 0.2453 - val_auc: 0.7210 - val_prc: 0.2723\n",
      "Epoch 4/10\n",
      "4836/4836 [==============================] - 171s 35ms/step - loss: 5.2211e-04 - tp: 155718.0000 - fp: 7.0000 - tn: 153702.0000 - fn: 59.0000 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9996 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1391 - val_tp: 85.0000 - val_fp: 45.0000 - val_tn: 12822.0000 - val_fn: 339.0000 - val_accuracy: 0.9711 - val_precision: 0.6538 - val_recall: 0.2005 - val_auc: 0.6858 - val_prc: 0.2626\n",
      "Epoch 5/10\n",
      "4835/4836 [============================>.] - ETA: 0s - loss: 6.5674e-04 - tp: 155654.0000 - fp: 11.0000 - tn: 153673.0000 - fn: 102.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "4836/4836 [==============================] - 171s 35ms/step - loss: 6.5667e-04 - tp: 155675.0000 - fp: 11.0000 - tn: 153698.0000 - fn: 102.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1544 - val_tp: 78.0000 - val_fp: 37.0000 - val_tn: 12830.0000 - val_fn: 346.0000 - val_accuracy: 0.9712 - val_precision: 0.6783 - val_recall: 0.1840 - val_auc: 0.6358 - val_prc: 0.2526\n",
      "Epoch 00005: early stopping\n",
      "INFO:tensorflow:Assets written to: model/result/model5_2/assets\n",
      "3\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 600, 128)          4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 549,441\n",
      "Trainable params: 549,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5020/5020 [==============================] - 180s 36ms/step - loss: 0.0290 - tp: 153766.0000 - fp: 280.0000 - tn: 159485.0000 - fn: 7739.0000 - accuracy: 0.9750 - precision: 0.9982 - recall: 0.9521 - auc: 0.9990 - prc: 0.9990 - val_loss: 0.2470 - val_tp: 89.0000 - val_fp: 13.0000 - val_tn: 6798.0000 - val_fn: 499.0000 - val_accuracy: 0.9308 - val_precision: 0.8725 - val_recall: 0.1514 - val_auc: 0.7761 - val_prc: 0.4277\n",
      "Epoch 2/10\n",
      "5020/5020 [==============================] - 178s 36ms/step - loss: 0.0025 - tp: 161053.0000 - fp: 46.0000 - tn: 159719.0000 - fn: 452.0000 - accuracy: 0.9984 - precision: 0.9997 - recall: 0.9972 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.2783 - val_tp: 79.0000 - val_fp: 13.0000 - val_tn: 6798.0000 - val_fn: 509.0000 - val_accuracy: 0.9294 - val_precision: 0.8587 - val_recall: 0.1344 - val_auc: 0.7529 - val_prc: 0.3995\n",
      "Epoch 3/10\n",
      "5020/5020 [==============================] - 179s 36ms/step - loss: 0.0012 - tp: 161313.0000 - fp: 26.0000 - tn: 159739.0000 - fn: 192.0000 - accuracy: 0.9993 - precision: 0.9998 - recall: 0.9988 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.3879 - val_tp: 54.0000 - val_fp: 14.0000 - val_tn: 6797.0000 - val_fn: 534.0000 - val_accuracy: 0.9259 - val_precision: 0.7941 - val_recall: 0.0918 - val_auc: 0.6866 - val_prc: 0.2941\n",
      "Epoch 4/10\n",
      "5020/5020 [==============================] - ETA: 0s - loss: 8.9488e-04 - tp: 161384.0000 - fp: 13.0000 - tn: 159752.0000 - fn: 121.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "5020/5020 [==============================] - 181s 36ms/step - loss: 8.9488e-04 - tp: 161384.0000 - fp: 13.0000 - tn: 159752.0000 - fn: 121.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9993 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.2311 - val_tp: 44.0000 - val_fp: 14.0000 - val_tn: 6797.0000 - val_fn: 544.0000 - val_accuracy: 0.9246 - val_precision: 0.7586 - val_recall: 0.0748 - val_auc: 0.7635 - val_prc: 0.3763\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: model/result/model5_3/assets\n",
      "4\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 600, 128)          4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 549,441\n",
      "Trainable params: 549,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3979/3979 [==============================] - 152s 38ms/step - loss: 0.0477 - tp: 117611.0000 - fp: 404.0000 - tn: 125744.0000 - fn: 10881.0000 - accuracy: 0.9557 - precision: 0.9966 - recall: 0.9153 - auc: 0.9975 - prc: 0.9976 - val_loss: 0.0277 - val_tp: 115.0000 - val_fp: 1249.0000 - val_tn: 39179.0000 - val_fn: 171.0000 - val_accuracy: 0.9651 - val_precision: 0.0843 - val_recall: 0.4021 - val_auc: 0.8981 - val_prc: 0.2164\n",
      "Epoch 2/10\n",
      "3979/3979 [==============================] - 160s 40ms/step - loss: 0.0036 - tp: 127924.0000 - fp: 42.0000 - tn: 126106.0000 - fn: 568.0000 - accuracy: 0.9976 - precision: 0.9997 - recall: 0.9956 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0485 - val_tp: 80.0000 - val_fp: 1281.0000 - val_tn: 39147.0000 - val_fn: 206.0000 - val_accuracy: 0.9635 - val_precision: 0.0588 - val_recall: 0.2797 - val_auc: 0.8832 - val_prc: 0.0516\n",
      "Epoch 3/10\n",
      "3979/3979 [==============================] - 152s 38ms/step - loss: 0.0017 - tp: 128255.0000 - fp: 20.0000 - tn: 126128.0000 - fn: 237.0000 - accuracy: 0.9990 - precision: 0.9998 - recall: 0.9982 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0284 - val_tp: 114.0000 - val_fp: 1088.0000 - val_tn: 39340.0000 - val_fn: 172.0000 - val_accuracy: 0.9691 - val_precision: 0.0948 - val_recall: 0.3986 - val_auc: 0.9207 - val_prc: 0.0978\n",
      "Epoch 4/10\n",
      "3979/3979 [==============================] - ETA: 0s - loss: 6.0709e-04 - tp: 128412.0000 - fp: 6.0000 - tn: 126142.0000 - fn: 80.0000 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9994 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "3979/3979 [==============================] - 149s 37ms/step - loss: 6.0709e-04 - tp: 128412.0000 - fp: 6.0000 - tn: 126142.0000 - fn: 80.0000 - accuracy: 0.9997 - precision: 1.0000 - recall: 0.9994 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0367 - val_tp: 116.0000 - val_fp: 888.0000 - val_tn: 39540.0000 - val_fn: 170.0000 - val_accuracy: 0.9740 - val_precision: 0.1155 - val_recall: 0.4056 - val_auc: 0.9098 - val_prc: 0.1578\n",
      "Epoch 00004: early stopping\n",
      "INFO:tensorflow:Assets written to: model/result/model5_4/assets\n",
      "5\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 600, 128)          4224      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 150, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 75, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 75, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 37, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4736)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                151584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 549,441\n",
      "Trainable params: 549,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4076/4076 [==============================] - 155s 38ms/step - loss: 0.0444 - tp: 122134.0000 - fp: 442.0000 - tn: 128994.0000 - fn: 9286.0000 - accuracy: 0.9627 - precision: 0.9964 - recall: 0.9293 - auc: 0.9979 - prc: 0.9980 - val_loss: 0.0594 - val_tp: 77.0000 - val_fp: 1800.0000 - val_tn: 35340.0000 - val_fn: 389.0000 - val_accuracy: 0.9418 - val_precision: 0.0410 - val_recall: 0.1652 - val_auc: 0.5837 - val_prc: 0.1218\n",
      "Epoch 2/10\n",
      "4076/4076 [==============================] - 153s 37ms/step - loss: 0.0036 - tp: 130888.0000 - fp: 56.0000 - tn: 129380.0000 - fn: 532.0000 - accuracy: 0.9977 - precision: 0.9996 - recall: 0.9960 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1277 - val_tp: 112.0000 - val_fp: 4403.0000 - val_tn: 32737.0000 - val_fn: 354.0000 - val_accuracy: 0.8735 - val_precision: 0.0248 - val_recall: 0.2403 - val_auc: 0.5959 - val_prc: 0.1162\n",
      "Epoch 3/10\n",
      "4076/4076 [==============================] - 154s 38ms/step - loss: 0.0015 - tp: 131195.0000 - fp: 21.0000 - tn: 129415.0000 - fn: 225.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9983 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0639 - val_tp: 137.0000 - val_fp: 1981.0000 - val_tn: 35159.0000 - val_fn: 329.0000 - val_accuracy: 0.9386 - val_precision: 0.0647 - val_recall: 0.2940 - val_auc: 0.6267 - val_prc: 0.1402\n",
      "Epoch 4/10\n",
      "4076/4076 [==============================] - 153s 38ms/step - loss: 7.6925e-04 - tp: 131318.0000 - fp: 13.0000 - tn: 129423.0000 - fn: 102.0000 - accuracy: 0.9996 - precision: 0.9999 - recall: 0.9992 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1415 - val_tp: 128.0000 - val_fp: 4727.0000 - val_tn: 32413.0000 - val_fn: 338.0000 - val_accuracy: 0.8653 - val_precision: 0.0264 - val_recall: 0.2747 - val_auc: 0.6197 - val_prc: 0.1409\n",
      "Epoch 5/10\n",
      "4076/4076 [==============================] - 153s 38ms/step - loss: 4.7763e-04 - tp: 131371.0000 - fp: 7.0000 - tn: 129429.0000 - fn: 49.0000 - accuracy: 0.9998 - precision: 0.9999 - recall: 0.9996 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0983 - val_tp: 82.0000 - val_fp: 1572.0000 - val_tn: 35568.0000 - val_fn: 384.0000 - val_accuracy: 0.9480 - val_precision: 0.0496 - val_recall: 0.1760 - val_auc: 0.5879 - val_prc: 0.1120\n",
      "Epoch 6/10\n",
      "4076/4076 [==============================] - 153s 38ms/step - loss: 6.6593e-04 - tp: 131343.0000 - fp: 8.0000 - tn: 129428.0000 - fn: 77.0000 - accuracy: 0.9997 - precision: 0.9999 - recall: 0.9994 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0835 - val_tp: 100.0000 - val_fp: 1666.0000 - val_tn: 35474.0000 - val_fn: 366.0000 - val_accuracy: 0.9460 - val_precision: 0.0566 - val_recall: 0.2146 - val_auc: 0.6147 - val_prc: 0.1396\n",
      "Epoch 7/10\n",
      "4075/4076 [============================>.] - ETA: 0s - loss: 4.2305e-04 - tp: 131354.0000 - fp: 6.0000 - tn: 129402.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "4076/4076 [==============================] - 153s 38ms/step - loss: 4.2296e-04 - tp: 131382.0000 - fp: 6.0000 - tn: 129430.0000 - fn: 38.0000 - accuracy: 0.9998 - precision: 1.0000 - recall: 0.9997 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.1132 - val_tp: 84.0000 - val_fp: 2040.0000 - val_tn: 35100.0000 - val_fn: 382.0000 - val_accuracy: 0.9356 - val_precision: 0.0395 - val_recall: 0.1803 - val_auc: 0.5743 - val_prc: 0.1091\n",
      "Epoch 00007: early stopping\n",
      "INFO:tensorflow:Assets written to: model/result/model5_5/assets\n"
     ]
    }
   ],
   "source": [
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=custom_padding(data_train, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=custom_padding(data_valid, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model=custom_model(seq_length=max_input_length, dropout_rate=dropout_rate)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es], class_weight={0: 10, 1: 1})\n",
    "\n",
    "    model.save('model/result/'+model_name+'_'+str(fold))\n",
    "    with open('model/result/'+model_name+'_'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "647c8887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "model2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "model3\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "model4\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "model5\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>134.0</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>17896.6</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>0.320228</td>\n",
       "      <td>0.400119</td>\n",
       "      <td>0.771427</td>\n",
       "      <td>0.268441</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>79.2</td>\n",
       "      <td>555.4</td>\n",
       "      <td>20427.8</td>\n",
       "      <td>555.4</td>\n",
       "      <td>0.415922</td>\n",
       "      <td>0.256150</td>\n",
       "      <td>0.786700</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>59.4</td>\n",
       "      <td>379.2</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>379.2</td>\n",
       "      <td>0.500499</td>\n",
       "      <td>0.205998</td>\n",
       "      <td>0.675733</td>\n",
       "      <td>0.228969</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>95.2</td>\n",
       "      <td>688.4</td>\n",
       "      <td>20294.8</td>\n",
       "      <td>688.4</td>\n",
       "      <td>0.283132</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.712844</td>\n",
       "      <td>0.226519</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>86.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>19781.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>0.263498</td>\n",
       "      <td>0.787015</td>\n",
       "      <td>0.313233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tp      fp       tn      fn  precision    recall       auc  \\\n",
       "model                                                                   \n",
       "model1  134.0  3086.6  17896.6  3086.6   0.320228  0.400119  0.771427   \n",
       "model2   79.2   555.4  20427.8   555.4   0.415922  0.256150  0.786700   \n",
       "model3   59.4   379.2  20604.0   379.2   0.500499  0.205998  0.675733   \n",
       "model4   95.2   688.4  20294.8   688.4   0.283132  0.290720  0.712844   \n",
       "model5   86.2  1202.0  19781.2  1202.0   0.546717  0.263498  0.787015   \n",
       "\n",
       "             prc  fold  \n",
       "model                   \n",
       "model1  0.268441     3  \n",
       "model2  0.268462     3  \n",
       "model3  0.228969     3  \n",
       "model4  0.226519     3  \n",
       "model5  0.313233     3  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp=[]\n",
    "fp=[]\n",
    "tn=[]\n",
    "fn=[]\n",
    "precision=[]\n",
    "recall=[]\n",
    "auc=[]\n",
    "prc=[]\n",
    "fold = []\n",
    "model = []\n",
    "\n",
    "for i in range(1,6):\n",
    "    m='model' + str(i)\n",
    "    print(m)\n",
    "    for j in range(1,6):\n",
    "        print(j)\n",
    "        f = open('model/result/'+ m +'_'+str(j)+'.json')\n",
    "        result = json.load(f)\n",
    "        f.close()\n",
    "        max_prc = max(result['val_prc'])\n",
    "        index_max = result['val_prc'].index(max_prc)\n",
    "        tp.append(result['val_tp'][index_max])\n",
    "        fp.append(result['val_fp'][index_max])\n",
    "        tn.append(result['val_tn'][index_max])\n",
    "        fn.append(result['val_fp'][index_max])\n",
    "        precision.append(result['val_precision'][index_max])\n",
    "        recall.append(result['val_recall'][index_max])\n",
    "        auc.append(result['val_auc'][index_max])\n",
    "        prc.append(max_prc)\n",
    "        fold.append(j)\n",
    "        model.append(m)\n",
    "\n",
    "result=pd.DataFrame({\"tp\":tp,\n",
    "                     \"fp\":fp,\n",
    "                     \"tn\":tn,\n",
    "                     \"fn\":fn,\n",
    "                     \"precision\":precision,\n",
    "                     \"recall\":recall,\n",
    "                     \"auc\":auc,\n",
    "                     \"prc\":prc,\n",
    "                     \"fold\":fold,\n",
    "                     \"model\":model\n",
    "                    })\n",
    "\n",
    "result.groupby(['model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "62c4679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>134.0</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>17896.6</td>\n",
       "      <td>3086.6</td>\n",
       "      <td>0.320228</td>\n",
       "      <td>0.400119</td>\n",
       "      <td>0.771427</td>\n",
       "      <td>0.268441</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>79.2</td>\n",
       "      <td>555.4</td>\n",
       "      <td>20427.8</td>\n",
       "      <td>555.4</td>\n",
       "      <td>0.415922</td>\n",
       "      <td>0.256150</td>\n",
       "      <td>0.786700</td>\n",
       "      <td>0.268462</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>59.4</td>\n",
       "      <td>379.2</td>\n",
       "      <td>20604.0</td>\n",
       "      <td>379.2</td>\n",
       "      <td>0.500499</td>\n",
       "      <td>0.205998</td>\n",
       "      <td>0.675733</td>\n",
       "      <td>0.228969</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>95.2</td>\n",
       "      <td>688.4</td>\n",
       "      <td>20294.8</td>\n",
       "      <td>688.4</td>\n",
       "      <td>0.283132</td>\n",
       "      <td>0.290720</td>\n",
       "      <td>0.712844</td>\n",
       "      <td>0.226519</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>86.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>19781.2</td>\n",
       "      <td>1202.0</td>\n",
       "      <td>0.546717</td>\n",
       "      <td>0.263498</td>\n",
       "      <td>0.787015</td>\n",
       "      <td>0.313233</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tp      fp       tn      fn  precision    recall       auc  \\\n",
       "model                                                                   \n",
       "model1  134.0  3086.6  17896.6  3086.6   0.320228  0.400119  0.771427   \n",
       "model2   79.2   555.4  20427.8   555.4   0.415922  0.256150  0.786700   \n",
       "model3   59.4   379.2  20604.0   379.2   0.500499  0.205998  0.675733   \n",
       "model4   95.2   688.4  20294.8   688.4   0.283132  0.290720  0.712844   \n",
       "model5   86.2  1202.0  19781.2  1202.0   0.546717  0.263498  0.787015   \n",
       "\n",
       "             prc  fold  \n",
       "model                   \n",
       "model1  0.268441     3  \n",
       "model2  0.268462     3  \n",
       "model3  0.228969     3  \n",
       "model4  0.226519     3  \n",
       "model5  0.313233     3  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "866eb4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>144.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>12589.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0.303030</td>\n",
       "      <td>0.419811</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.382783</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>64.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>12823.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.242489</td>\n",
       "      <td>0.850701</td>\n",
       "      <td>0.298174</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>52.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>12821.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0.530612</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.662228</td>\n",
       "      <td>0.299596</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>117.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>12610.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0.135220</td>\n",
       "      <td>0.275943</td>\n",
       "      <td>0.789040</td>\n",
       "      <td>0.242010</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>89.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12847.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.797980</td>\n",
       "      <td>0.274678</td>\n",
       "      <td>0.776106</td>\n",
       "      <td>0.345711</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tp     fp       tn     fn  precision    recall       auc       prc  \\\n",
       "model                                                                           \n",
       "model1  144.0  278.0  12589.0  278.0   0.303030  0.419811  0.805340  0.382783   \n",
       "model2   64.0   44.0  12823.0   44.0   0.384615  0.242489  0.850701  0.298174   \n",
       "model3   52.0   46.0  12821.0   46.0   0.530612  0.122642  0.662228  0.299596   \n",
       "model4  117.0  311.0  12610.0  311.0   0.135220  0.275943  0.789040  0.242010   \n",
       "model5   89.0   20.0  12847.0   20.0   0.797980  0.274678  0.776106  0.345711   \n",
       "\n",
       "        fold  \n",
       "model         \n",
       "model1     3  \n",
       "model2     3  \n",
       "model3     3  \n",
       "model4     3  \n",
       "model5     3  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "84647118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>61.951594</td>\n",
       "      <td>4632.217374</td>\n",
       "      <td>12848.756683</td>\n",
       "      <td>4632.217374</td>\n",
       "      <td>0.343005</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.163010</td>\n",
       "      <td>0.207775</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>46.547825</td>\n",
       "      <td>970.236466</td>\n",
       "      <td>15805.283284</td>\n",
       "      <td>970.236466</td>\n",
       "      <td>0.298535</td>\n",
       "      <td>0.148268</td>\n",
       "      <td>0.105670</td>\n",
       "      <td>0.125340</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>41.180092</td>\n",
       "      <td>571.105244</td>\n",
       "      <td>15992.394974</td>\n",
       "      <td>571.105244</td>\n",
       "      <td>0.385722</td>\n",
       "      <td>0.154167</td>\n",
       "      <td>0.135825</td>\n",
       "      <td>0.122030</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>43.922659</td>\n",
       "      <td>807.351720</td>\n",
       "      <td>15818.055576</td>\n",
       "      <td>807.351720</td>\n",
       "      <td>0.332899</td>\n",
       "      <td>0.108256</td>\n",
       "      <td>0.127282</td>\n",
       "      <td>0.108027</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>41.889139</td>\n",
       "      <td>2042.123894</td>\n",
       "      <td>14993.240284</td>\n",
       "      <td>2042.123894</td>\n",
       "      <td>0.452341</td>\n",
       "      <td>0.099284</td>\n",
       "      <td>0.122620</td>\n",
       "      <td>0.130564</td>\n",
       "      <td>1.581139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tp           fp            tn           fn  precision  \\\n",
       "model                                                                  \n",
       "model1  61.951594  4632.217374  12848.756683  4632.217374   0.343005   \n",
       "model2  46.547825   970.236466  15805.283284   970.236466   0.298535   \n",
       "model3  41.180092   571.105244  15992.394974   571.105244   0.385722   \n",
       "model4  43.922659   807.351720  15818.055576   807.351720   0.332899   \n",
       "model5  41.889139  2042.123894  14993.240284  2042.123894   0.452341   \n",
       "\n",
       "          recall       auc       prc      fold  \n",
       "model                                           \n",
       "model1  0.104555  0.163010  0.207775  1.581139  \n",
       "model2  0.148268  0.105670  0.125340  1.581139  \n",
       "model3  0.154167  0.135825  0.122030  1.581139  \n",
       "model4  0.108256  0.127282  0.108027  1.581139  \n",
       "model5  0.099284  0.122620  0.130564  1.581139  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a97d11d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tp</th>\n",
       "      <th>fp</th>\n",
       "      <th>tn</th>\n",
       "      <th>fn</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auc</th>\n",
       "      <th>prc</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <td>184.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>36052.0</td>\n",
       "      <td>10688.0</td>\n",
       "      <td>0.858974</td>\n",
       "      <td>0.503497</td>\n",
       "      <td>0.949154</td>\n",
       "      <td>0.439929</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <td>138.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>40005.0</td>\n",
       "      <td>2264.0</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.482517</td>\n",
       "      <td>0.886486</td>\n",
       "      <td>0.386172</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <td>128.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>39923.0</td>\n",
       "      <td>1329.0</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.447552</td>\n",
       "      <td>0.888736</td>\n",
       "      <td>0.310905</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model4</th>\n",
       "      <td>129.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>39603.0</td>\n",
       "      <td>2033.0</td>\n",
       "      <td>0.849057</td>\n",
       "      <td>0.451049</td>\n",
       "      <td>0.822278</td>\n",
       "      <td>0.347924</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model5</th>\n",
       "      <td>128.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>39179.0</td>\n",
       "      <td>4727.0</td>\n",
       "      <td>0.952381</td>\n",
       "      <td>0.402098</td>\n",
       "      <td>0.913924</td>\n",
       "      <td>0.435514</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           tp       fp       tn       fn  precision    recall       auc  \\\n",
       "model                                                                     \n",
       "model1  184.0  10688.0  36052.0  10688.0   0.858974  0.503497  0.949154   \n",
       "model2  138.0   2264.0  40005.0   2264.0   0.820513  0.482517  0.886486   \n",
       "model3  128.0   1329.0  39923.0   1329.0   0.947368  0.447552  0.888736   \n",
       "model4  129.0   2033.0  39603.0   2033.0   0.849057  0.451049  0.822278   \n",
       "model5  128.0   4727.0  39179.0   4727.0   0.952381  0.402098  0.913924   \n",
       "\n",
       "             prc  fold  \n",
       "model                   \n",
       "model1  0.439929     5  \n",
       "model2  0.386172     5  \n",
       "model3  0.310905     5  \n",
       "model4  0.347924     5  \n",
       "model5  0.435514     5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.groupby(['model']).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1601dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    #training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "    #                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = word_embedding_layer(inputs)\n",
    "    outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=256, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/embedding16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/embedding16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ce92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263fe1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'model/transformer16_fold'+str(fold)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa411b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    #outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fe199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23aa6a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "fold=1\n",
    "data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "data_train=data_all[data_all['set']=='train']\n",
    "data_valid=data_all[data_all['set']=='valid']\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train=encode_padding(data_train, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_train=data_train['label']\n",
    "X_valid=encode_padding(data_valid, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_valid=data_valid['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b757189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 4  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 128  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cc7bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15595cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57918f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff50a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d324be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "#outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = word_embedding_layer(inputs)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1be48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = K.max(outputs,axis=-1)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a66b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = GroupShuffleSplit(test_size=.20, n_splits=2)\n",
    "\n",
    "while True:\n",
    "    split = valid_split.split(data, y, groups=g)\n",
    "    train_inds, valid_inds = next(split)\n",
    "    if len(train_inds)/len(valid_inds)>3:\n",
    "        break\n",
    "    \n",
    "data_train=data.iloc[train_inds,:]\n",
    "data_valid=data.iloc[valid_inds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9609d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175424f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8480efbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos=data_train[data_train.label==1]\n",
    "data_train_pos_new = data_train_pos.sample(n=data_train.shape[0], random_state=1, replace=True)\n",
    "\n",
    "for i in range(data_train_pos_new.shape[0]):\n",
    "    r=random.uniform(0, 1)\n",
    "    if r>0.75:\n",
    "        tmp=random.sample([-1, -2, -3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][:tmp[0]]\n",
    "    if r >0.5 and r <=0.75:\n",
    "        tmp=random.sample([1, 2, 3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][tmp[0]:]\n",
    "    if r >0.25 and r <=0.5:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i] + tmp\n",
    "    else:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=tmp + data_train_pos_new.seq.iloc[i]\n",
    "\n",
    "data_train_all = pd.concat([data_train, data_train_pos_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f43515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all = data_train_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb9a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all['set']='train'\n",
    "data_valid['set']='valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9277635",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train_all, data_valid])\n",
    "data_all.reset_index(drop=True)\n",
    "data_all['id']=str(fold)+'_'+data_all.index.astype(str)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1970b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('data/fold'+str(fold)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f64b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=one_hot_encode_padding(data_train_all, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_train=data_train_all['label']\n",
    "X_valid=one_hot_encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_valid=data_valid['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff27ef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7339c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(y_valid)\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(y_valid)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6eae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94c719a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = train_model(X_train, y_train, (X_valid, y_valid), channel=CFG.channel,\n",
    "                   nb_epoch=10, border_mode='same',\n",
    "                   inp_len=CFG.feature_seq_len, nodes=40, layers=5, nbr_filters=120, filter_len=8, dropout1=0,\n",
    "                   dropout2=0, dropout3=0.2, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db944b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51265db",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/IRESbase_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e060fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/RF00031.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd751c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc4871",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=one_hot_encode_padding(IRESbase, col='IRES.Sequence', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b31344",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba9add1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896731bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421d37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 8  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 8  # Dimensionality of the linearly projected values\n",
    "d_ff = 8  # Dimensionality of the inner fully connected layer\n",
    "d_model = 8  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f68bd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab634b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc11ab70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_padding(df, col='utr', seq_len=50, padding='5end', channel=1):\n",
    "    # 5end padding means pad the left end (5' end) if sequence length < seq_len; keep the seq_len right end (3' end) if  sequence length > seq_len\n",
    "    # 3end padding means pad the right end (3' end) if sequence length < seq_len; keep the seq_len left end (5' end) if  sequence length > seq_len\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1],'c':[2],'g':[3],'t':[4], 'n':[0], '(':[5],')':[6],'.':[7]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.zeros([len(df),seq_len,channel])\n",
    "    \n",
    "    # Iterate through UTRs and one-hot encode\n",
    "    for i,seq in enumerate(df[col]):\n",
    "        if(isinstance(seq, str)):\n",
    "            if(padding=='3end'):\n",
    "                seq=seq[:min(len(seq),seq_len)]\n",
    "            if(padding=='5end'):\n",
    "                seq=seq[max(0,(len(seq)-seq_len)):len(seq)]\n",
    "            seq = seq.lower()\n",
    "            a = np.array([nuc_d[x] for x in seq])\n",
    "            if(padding=='5end'):\n",
    "                vectors[i, (seq_len-len(seq)):seq_len] = a\n",
    "            if(padding=='3end'):\n",
    "                vectors[i, :len(seq)] = a\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e890e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.squeeze(encode_padding(data_train_all, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_train=data_train_all['label']\n",
    "X_valid=np.squeeze(encode_padding(data_valid, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_valid=data_valid['label']\n",
    "#m_valid=np.squeeze(tf.math.equal(X_valid, 0))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfba287",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ea825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f44efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 64 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 64 # Vocabulary size for the decoder\n",
    " \n",
    "enc_seq_length = 200  # Maximum length of the input sequence\n",
    "dec_seq_length = 200  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 64  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e152c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef258a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c0f336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    " \n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    " \n",
    "    def __call__(self, step_num):\n",
    " \n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    " \n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f20a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "      keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "      keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "      keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "          batch_size=256, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044b08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecb655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16b5aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18adc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(IRESbase['IRES.Sequence'].to_numpy(), \n",
    "                                              value='N', padding='post', truncating='post', maxlen=600, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94903d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.TextVectorization('ATCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44977c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd14d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = [\"A\", \"C\", \"G\", \"T\"]\n",
    "max_len = CFG.feature_seq_len\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len,\n",
    " vocabulary=vocab_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79067720",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t  = Tokenizer(num_words=5,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=False, char_level=True, oov_token=None,\n",
    "    document_count=0)\n",
    "fit_text = \"ACGTN\"\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "test_text = \"NCGTA\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44f21dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid['seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=t.texts_to_sequences(data_valid['seq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1aa6319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        import numpy as np\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650f741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc37a26d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc1750",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd1769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7763d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8483db",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287bcd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dfb25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "with open('nucl.json') as f:\n",
    "    contents = f.readlines()\n",
    "    t = tokenizer_from_json(contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a73c853",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b466c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b07a4092d17fabcc770aac63fd8b04ec6822179f9cd74920d0d99ada38e73abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
