{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aae94ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, convert_to_tensor, string\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow import linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, MaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.layers import Activation, Flatten, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import softmax\n",
    "\n",
    "from focal_loss import BinaryFocalLoss\n",
    "from functions import LRScheduler, TransformerModel \n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Parameters for plotting model results ###\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.labelpad'] = 5\n",
    "plt.rcParams['axes.linewidth']= 2\n",
    "plt.rcParams['xtick.labelsize']= 14\n",
    "plt.rcParams['ytick.labelsize']= 14\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.minor.size'] = 3\n",
    "plt.rcParams['ytick.minor.size'] = 3\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9be2755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6689f3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.3.4\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version: ' + tf. __version__)\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b026711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic random seed\n",
    "DEFAULT_RANDOM_SEED = 2022\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# tensorflow random seed \n",
    "def seedTF(seed=DEFAULT_RANDOM_SEED):\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "# import torch\n",
    "# def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + tensorflow + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTF(seed)\n",
    "    # seedTorch(seed)\n",
    "\n",
    "seedEverything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0312ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='model2'\n",
    "#padding_method = 'encode_padding'\n",
    "#channel = 1\n",
    "padding_method = 'one_hot_encode_padding'\n",
    "channel = 4\n",
    "max_input_length = 600\n",
    "dropout_rate = 0.1\n",
    "feature_pad_end= '3end'\n",
    "\n",
    "import models\n",
    "custom_model = getattr(models, model_name)\n",
    "import functions\n",
    "custom_padding = getattr(functions, padding_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c89fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "5010/5010 [==============================] - 300s 60ms/step - loss: 0.0428 - tp: 150419.0000 - fp: 484.0000 - tn: 158422.0000 - fn: 11271.0000 - accuracy: 0.9633 - precision: 0.9968 - recall: 0.9303 - auc: 0.9979 - prc: 0.9980 - val_loss: 0.0271 - val_tp: 10.0000 - val_fp: 57.0000 - val_tn: 7613.0000 - val_fn: 56.0000 - val_accuracy: 0.9854 - val_precision: 0.1493 - val_recall: 0.1515 - val_auc: 0.6886 - val_prc: 0.0762\n",
      "Epoch 2/10\n",
      "5010/5010 [==============================] - 298s 60ms/step - loss: 0.0050 - tp: 160708.0000 - fp: 91.0000 - tn: 158815.0000 - fn: 982.0000 - accuracy: 0.9967 - precision: 0.9994 - recall: 0.9939 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0304 - val_tp: 8.0000 - val_fp: 7.0000 - val_tn: 7663.0000 - val_fn: 58.0000 - val_accuracy: 0.9916 - val_precision: 0.5333 - val_recall: 0.1212 - val_auc: 0.5819 - val_prc: 0.1673\n",
      "Epoch 3/10\n",
      "5010/5010 [==============================] - 298s 59ms/step - loss: 0.0032 - tp: 161118.0000 - fp: 56.0000 - tn: 158850.0000 - fn: 572.0000 - accuracy: 0.9980 - precision: 0.9997 - recall: 0.9965 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0258 - val_tp: 20.0000 - val_fp: 32.0000 - val_tn: 7638.0000 - val_fn: 46.0000 - val_accuracy: 0.9899 - val_precision: 0.3846 - val_recall: 0.3030 - val_auc: 0.8512 - val_prc: 0.3210\n",
      "Epoch 4/10\n",
      "5010/5010 [==============================] - 299s 60ms/step - loss: 0.0017 - tp: 161392.0000 - fp: 30.0000 - tn: 158876.0000 - fn: 298.0000 - accuracy: 0.9990 - precision: 0.9998 - recall: 0.9982 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0192 - val_tp: 19.0000 - val_fp: 15.0000 - val_tn: 7655.0000 - val_fn: 47.0000 - val_accuracy: 0.9920 - val_precision: 0.5588 - val_recall: 0.2879 - val_auc: 0.8011 - val_prc: 0.2883\n",
      "Epoch 5/10\n",
      "5010/5010 [==============================] - 297s 59ms/step - loss: 0.0023 - tp: 161252.0000 - fp: 34.0000 - tn: 158872.0000 - fn: 438.0000 - accuracy: 0.9985 - precision: 0.9998 - recall: 0.9973 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0151 - val_tp: 17.0000 - val_fp: 50.0000 - val_tn: 7620.0000 - val_fn: 49.0000 - val_accuracy: 0.9872 - val_precision: 0.2537 - val_recall: 0.2576 - val_auc: 0.8730 - val_prc: 0.1951\n",
      "Epoch 6/10\n",
      "5009/5010 [============================>.] - ETA: 0s - loss: 0.0011 - tp: 161508.0000 - fp: 13.0000 - tn: 158880.0000 - fn: 175.0000 - accuracy: 0.9994 - precision: 0.9999 - recall: 0.9989 - auc: 1.0000 - prc: 1.0000Restoring model weights from the end of the best epoch.\n",
      "5010/5010 [==============================] - 297s 59ms/step - loss: 0.0011 - tp: 161515.0000 - fp: 13.0000 - tn: 158893.0000 - fn: 175.0000 - accuracy: 0.9994 - precision: 0.9999 - recall: 0.9989 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0473 - val_tp: 11.0000 - val_fp: 12.0000 - val_tn: 7658.0000 - val_fn: 55.0000 - val_accuracy: 0.9913 - val_precision: 0.4783 - val_recall: 0.1667 - val_auc: 0.6414 - val_prc: 0.2003\n",
      "Epoch 00006: early stopping\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model/result/model2_1/assets\n",
      "2\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 593, 128)          4224      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 586, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 293, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 293, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 286, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 279, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 139, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 139, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 132, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 55, 128)           131200    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 48, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                98336     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,020,993\n",
      "Trainable params: 1,020,993\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "4836/4836 [==============================] - 299s 62ms/step - loss: 0.0390 - tp: 145654.0000 - fp: 409.0000 - tn: 153300.0000 - fn: 10123.0000 - accuracy: 0.9660 - precision: 0.9972 - recall: 0.9350 - auc: 0.9983 - prc: 0.9983 - val_loss: 0.0868 - val_tp: 37.0000 - val_fp: 91.0000 - val_tn: 12776.0000 - val_fn: 387.0000 - val_accuracy: 0.9640 - val_precision: 0.2891 - val_recall: 0.0873 - val_auc: 0.8356 - val_prc: 0.2318\n",
      "Epoch 2/10\n",
      " 344/4836 [=>............................] - ETA: 4:52 - loss: 0.0021 - tp: 11060.0000 - fp: 2.0000 - tn: 10915.0000 - fn: 39.0000 - accuracy: 0.9981 - precision: 0.9998 - recall: 0.9965 - auc: 1.0000 - prc: 1.0000"
     ]
    }
   ],
   "source": [
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=custom_padding(data_train, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=custom_padding(data_valid, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model=custom_model(seq_length=max_input_length, dropout_rate=dropout_rate)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es], class_weight={0: 10, 1: 1})\n",
    "\n",
    "    model.save('model/result/'+model_name+'_'+str(fold))\n",
    "    with open('model/result/'+model_name+'_'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3d50d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d8bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7ae037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    #training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "    #                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = word_embedding_layer(inputs)\n",
    "    outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=256, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/embedding16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/embedding16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b771374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a6fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'model/transformer16_fold'+str(fold)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cef254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    #outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c1d0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "fold=1\n",
    "data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "data_train=data_all[data_all['set']=='train']\n",
    "data_valid=data_all[data_all['set']=='valid']\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train=encode_padding(data_train, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_train=data_train['label']\n",
    "X_valid=encode_padding(data_valid, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_valid=data_valid['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652bb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 4  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 128  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5fe18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf5893",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b5badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760404bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa912f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "#outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = word_embedding_layer(inputs)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e19c153",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = K.max(outputs,axis=-1)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = GroupShuffleSplit(test_size=.20, n_splits=2)\n",
    "\n",
    "while True:\n",
    "    split = valid_split.split(data, y, groups=g)\n",
    "    train_inds, valid_inds = next(split)\n",
    "    if len(train_inds)/len(valid_inds)>3:\n",
    "        break\n",
    "    \n",
    "data_train=data.iloc[train_inds,:]\n",
    "data_valid=data.iloc[valid_inds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a97357",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9346f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75588e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos=data_train[data_train.label==1]\n",
    "data_train_pos_new = data_train_pos.sample(n=data_train.shape[0], random_state=1, replace=True)\n",
    "\n",
    "for i in range(data_train_pos_new.shape[0]):\n",
    "    r=random.uniform(0, 1)\n",
    "    if r>0.75:\n",
    "        tmp=random.sample([-1, -2, -3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][:tmp[0]]\n",
    "    if r >0.5 and r <=0.75:\n",
    "        tmp=random.sample([1, 2, 3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][tmp[0]:]\n",
    "    if r >0.25 and r <=0.5:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i] + tmp\n",
    "    else:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=tmp + data_train_pos_new.seq.iloc[i]\n",
    "\n",
    "data_train_all = pd.concat([data_train, data_train_pos_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe63e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all = data_train_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a92f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all['set']='train'\n",
    "data_valid['set']='valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26fd516",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e7ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train_all, data_valid])\n",
    "data_all.reset_index(drop=True)\n",
    "data_all['id']=str(fold)+'_'+data_all.index.astype(str)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1646df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('data/fold'+str(fold)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1206d8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=one_hot_encode_padding(data_train_all, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_train=data_train_all['label']\n",
    "X_valid=one_hot_encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_valid=data_valid['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40cd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7503e65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(y_valid)\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(y_valid)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5798879",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37cdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = train_model(X_train, y_train, (X_valid, y_valid), channel=CFG.channel,\n",
    "                   nb_epoch=10, border_mode='same',\n",
    "                   inp_len=CFG.feature_seq_len, nodes=40, layers=5, nbr_filters=120, filter_len=8, dropout1=0,\n",
    "                   dropout2=0, dropout3=0.2, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f731ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755bed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/IRESbase_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e85987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/RF00031.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4ac033",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c3ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=one_hot_encode_padding(IRESbase, col='IRES.Sequence', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413fb00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e6a1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f1b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8619be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2137ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb512315",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45939142",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 8  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 8  # Dimensionality of the linearly projected values\n",
    "d_ff = 8  # Dimensionality of the inner fully connected layer\n",
    "d_model = 8  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ff5061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca6829",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f963e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b71a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_padding(df, col='utr', seq_len=50, padding='5end', channel=1):\n",
    "    # 5end padding means pad the left end (5' end) if sequence length < seq_len; keep the seq_len right end (3' end) if  sequence length > seq_len\n",
    "    # 3end padding means pad the right end (3' end) if sequence length < seq_len; keep the seq_len left end (5' end) if  sequence length > seq_len\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1],'c':[2],'g':[3],'t':[4], 'n':[0], '(':[5],')':[6],'.':[7]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.zeros([len(df),seq_len,channel])\n",
    "    \n",
    "    # Iterate through UTRs and one-hot encode\n",
    "    for i,seq in enumerate(df[col]):\n",
    "        if(isinstance(seq, str)):\n",
    "            if(padding=='3end'):\n",
    "                seq=seq[:min(len(seq),seq_len)]\n",
    "            if(padding=='5end'):\n",
    "                seq=seq[max(0,(len(seq)-seq_len)):len(seq)]\n",
    "            seq = seq.lower()\n",
    "            a = np.array([nuc_d[x] for x in seq])\n",
    "            if(padding=='5end'):\n",
    "                vectors[i, (seq_len-len(seq)):seq_len] = a\n",
    "            if(padding=='3end'):\n",
    "                vectors[i, :len(seq)] = a\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca4889",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.squeeze(encode_padding(data_train_all, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_train=data_train_all['label']\n",
    "X_valid=np.squeeze(encode_padding(data_valid, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_valid=data_valid['label']\n",
    "#m_valid=np.squeeze(tf.math.equal(X_valid, 0))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109017a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a5cd14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 64 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 64 # Vocabulary size for the decoder\n",
    " \n",
    "enc_seq_length = 200  # Maximum length of the input sequence\n",
    "dec_seq_length = 200  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 64  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea92c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d2538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    " \n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    " \n",
    "    def __call__(self, step_num):\n",
    " \n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    " \n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547036e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "      keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "      keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "      keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "          batch_size=256, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd8b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0adf75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83731831",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14820c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(IRESbase['IRES.Sequence'].to_numpy(), \n",
    "                                              value='N', padding='post', truncating='post', maxlen=600, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4cfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.TextVectorization('ATCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871227a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab4240",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = [\"A\", \"C\", \"G\", \"T\"]\n",
    "max_len = CFG.feature_seq_len\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len,\n",
    " vocabulary=vocab_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0177e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t  = Tokenizer(num_words=5,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=False, char_level=True, oov_token=None,\n",
    "    document_count=0)\n",
    "fit_text = \"ACGTN\"\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "test_text = \"NCGTA\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595df23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid['seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31baa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=t.texts_to_sequences(data_valid['seq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1168373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        import numpy as np\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e0a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a59e273",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7521cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffac780",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d6b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04a5f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1abeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01726bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "with open('nucl.json') as f:\n",
    "    contents = f.readlines()\n",
    "    t = tokenizer_from_json(contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0d4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9b5568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b07a4092d17fabcc770aac63fd8b04ec6822179f9cd74920d0d99ada38e73abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
