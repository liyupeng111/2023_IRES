{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a33bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras, convert_to_tensor, string\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow import linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Layer, Embedding, MaxPooling1D\n",
    "from tensorflow.keras.layers import LayerNormalization, ReLU, Dropout\n",
    "from tensorflow.keras.layers import Activation, Flatten, Conv1D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.backend import softmax\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "### Parameters for plotting model results ###\n",
    "pd.set_option(\"display.max_colwidth\",100)\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['axes.labelweight'] = 'normal'\n",
    "plt.rcParams['axes.labelpad'] = 5\n",
    "plt.rcParams['axes.linewidth']= 2\n",
    "plt.rcParams['xtick.labelsize']= 14\n",
    "plt.rcParams['ytick.labelsize']= 14\n",
    "plt.rcParams['xtick.major.size'] = 6\n",
    "plt.rcParams['ytick.major.size'] = 6\n",
    "plt.rcParams['xtick.minor.size'] = 3\n",
    "plt.rcParams['ytick.minor.size'] = 3\n",
    "plt.rcParams['xtick.minor.width'] = 1\n",
    "plt.rcParams['ytick.minor.width'] = 1\n",
    "plt.rcParams['xtick.major.width'] = 2\n",
    "plt.rcParams['ytick.major.width'] = 2\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['axes.edgecolor'] = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae88c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting focal-loss\n",
      "  Downloading focal_loss-0.0.7-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: tensorflow>=2.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from focal-loss) (2.3.4)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.12.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.42.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.18.5)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.0.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.1.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (3.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (0.36.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.1.0)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (3.15.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (0.2.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.6.3)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (2.10.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (0.3.3)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (2.7.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorflow>=2.2->focal-loss) (1.15.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (2.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (1.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (0.6.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (49.6.0.post20210108)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (2.26.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (4.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (2.0.9)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from werkzeug>=0.11.15->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (0.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (3.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/amazonei_tensorflow2_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow>=2.2->focal-loss) (3.1.1)\n",
      "Installing collected packages: focal-loss\n",
      "Successfully installed focal-loss-0.0.7\n"
     ]
    }
   ],
   "source": [
    "! pip install focal-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c08f868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from focal_loss import BinaryFocalLoss\n",
    "from functions import LRScheduler, TransformerModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "957cb742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.3.4\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print('tensorflow version: ' + tf. __version__)\n",
    "print(tf.config.list_physical_devices('CPU'))\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d5bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic random seed\n",
    "DEFAULT_RANDOM_SEED = 2022\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# tensorflow random seed \n",
    "def seedTF(seed=DEFAULT_RANDOM_SEED):\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "# import torch\n",
    "# def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed(seed)\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + tensorflow + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTF(seed)\n",
    "    # seedTorch(seed)\n",
    "\n",
    "seedEverything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e51c60ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='model3'\n",
    "padding_method = 'encode_padding'\n",
    "channel = 1\n",
    "#padding_method = 'one_hot_encode_padding'\n",
    "#channel = 4\n",
    "max_input_length = 600\n",
    "dropout_rate = 0.1\n",
    "feature_pad_end= '3end'\n",
    "\n",
    "import models\n",
    "custom_model = getattr(models, model_name)\n",
    "import functions\n",
    "custom_padding = getattr(functions, padding_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7edbd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 600)]             0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 600, 16)           80        \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 593, 128)          16512     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 586, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 293, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 293, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 286, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 279, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 139, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 139, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 132, 128)          131200    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 125, 128)          131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 62, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 55, 128)           131200    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 48, 128)           131200    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 24, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                98336     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,033,361\n",
      "Trainable params: 1,033,361\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5010/5010 [==============================] - 326s 65ms/step - loss: 0.0583 - tp: 145508.0000 - fp: 597.0000 - tn: 158309.0000 - fn: 16182.0000 - accuracy: 0.9477 - precision: 0.9959 - recall: 0.8999 - auc: 0.9960 - prc: 0.9963 - val_loss: 0.0410 - val_tp: 18.0000 - val_fp: 125.0000 - val_tn: 7545.0000 - val_fn: 48.0000 - val_accuracy: 0.9776 - val_precision: 0.1259 - val_recall: 0.2727 - val_auc: 0.5332 - val_prc: 0.2428\n",
      "Epoch 2/10\n",
      "5010/5010 [==============================] - 326s 65ms/step - loss: 0.0069 - tp: 160357.0000 - fp: 109.0000 - tn: 158797.0000 - fn: 1333.0000 - accuracy: 0.9955 - precision: 0.9993 - recall: 0.9918 - auc: 0.9999 - prc: 0.9999 - val_loss: 0.0431 - val_tp: 19.0000 - val_fp: 132.0000 - val_tn: 7538.0000 - val_fn: 47.0000 - val_accuracy: 0.9769 - val_precision: 0.1258 - val_recall: 0.2879 - val_auc: 0.5328 - val_prc: 0.2851\n",
      "Epoch 3/10\n",
      "5010/5010 [==============================] - 323s 64ms/step - loss: 0.0040 - tp: 161008.0000 - fp: 72.0000 - tn: 158834.0000 - fn: 682.0000 - accuracy: 0.9976 - precision: 0.9996 - recall: 0.9958 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0437 - val_tp: 2.0000 - val_fp: 15.0000 - val_tn: 7655.0000 - val_fn: 64.0000 - val_accuracy: 0.9898 - val_precision: 0.1176 - val_recall: 0.0303 - val_auc: 0.6221 - val_prc: 0.0480\n",
      "Epoch 4/10\n",
      "5010/5010 [==============================] - 324s 65ms/step - loss: 0.0021 - tp: 161420.0000 - fp: 28.0000 - tn: 158878.0000 - fn: 270.0000 - accuracy: 0.9991 - precision: 0.9998 - recall: 0.9983 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0400 - val_tp: 14.0000 - val_fp: 1.0000 - val_tn: 7669.0000 - val_fn: 52.0000 - val_accuracy: 0.9931 - val_precision: 0.9333 - val_recall: 0.2121 - val_auc: 0.6186 - val_prc: 0.2791\n",
      "Epoch 5/10\n",
      "5010/5010 [==============================] - 324s 65ms/step - loss: 0.0025 - tp: 161271.0000 - fp: 38.0000 - tn: 158868.0000 - fn: 419.0000 - accuracy: 0.9986 - precision: 0.9998 - recall: 0.9974 - auc: 1.0000 - prc: 1.0000 - val_loss: 0.0434 - val_tp: 18.0000 - val_fp: 1.0000 - val_tn: 7669.0000 - val_fn: 48.0000 - val_accuracy: 0.9937 - val_precision: 0.9474 - val_recall: 0.2727 - val_auc: 0.6622 - val_prc: 0.3034\n",
      "Epoch 6/10\n",
      "1254/5010 [======>.......................] - ETA: 4:08 - loss: 9.7499e-04 - tp: 40511.0000 - fp: 6.0000 - tn: 39695.0000 - fn: 44.0000 - accuracy: 0.9994 - precision: 0.9999 - recall: 0.9989 - auc: 1.0000 - prc: 1.0000"
     ]
    }
   ],
   "source": [
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv.gz')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=custom_padding(data_train, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=custom_padding(data_valid, col='seq', seq_len=max_input_length, padding=feature_pad_end, channel=channel)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    model=custom_model(seq_length=max_input_length, dropout_rate=dropout_rate)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es], class_weight={0: 10, 1: 1})\n",
    "\n",
    "    model.save('model/result/'+model_name+'_'+str(fold))\n",
    "    with open('model/result/'+model_name+'_'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp=[]\n",
    "fp=[]\n",
    "tn=[]\n",
    "fn=[]\n",
    "auc=[]\n",
    "prc=[]\n",
    "fold = []\n",
    "model = []\n",
    "\n",
    "for i in range(1,3):\n",
    "    m='model' + str(i)\n",
    "    print(m)\n",
    "    for j in range(1,6):\n",
    "        print(j)\n",
    "        f = open('model/result/'+ m +'_'+str(j)+'.json')\n",
    "        result = json.load(f)\n",
    "        f.close()\n",
    "        max_prc = max(result['val_prc'])\n",
    "        index_max = result['val_prc'].index(max_prc)\n",
    "        tp.append(result['val_tp'][index_max])\n",
    "        fp.append(result['val_fp'][index_max])\n",
    "        tn.append(result['val_tn'][index_max])\n",
    "        fn.append(result['val_fp'][index_max])\n",
    "        auc.append(result['val_auc'][index_max])\n",
    "        prc.append(max_prc)\n",
    "        fold.append(j)\n",
    "        model.append(m)\n",
    "\n",
    "result=pd.DataFrame({\"tp\":tp,\n",
    "                     \"fp\":fp,\n",
    "                     \"tn\":tn,\n",
    "                     \"fn\":fn,\n",
    "                     \"auc\":auc,\n",
    "                     \"prc\":prc,\n",
    "                     \"fold\":fold,\n",
    "                     \"model\":model\n",
    "                    })\n",
    "\n",
    "result.groupby(['model']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8291e82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d19d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8af2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    #training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "    #                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = word_embedding_layer(inputs)\n",
    "    outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=256, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/embedding16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/embedding16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af2bd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d787fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'model/transformer16_fold'+str(fold)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c92c5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer + cnn\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 16  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "for fold in range(1,6):\n",
    "    print(fold)\n",
    "    data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "    data_train=data_all[data_all['set']=='train']\n",
    "    data_valid=data_all[data_all['set']=='valid']\n",
    "    \n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "    data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    X_train=encode_padding(data_train, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_train=data_train['label']\n",
    "    X_valid=encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=1)\n",
    "    y_valid=data_valid['label']\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "    training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "    outputs = training_model(inputs, training=True)\n",
    "    #outputs = K.max(outputs,axis=-1)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "    outputs = MaxPooling1D()(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Flatten()(outputs)\n",
    "    outputs = Dense(32)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    outputs = Dropout(dropout_rate)(outputs)\n",
    "    outputs = Dense(1)(outputs)\n",
    "    outputs = Activation('sigmoid')(outputs)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    #adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "    adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "    METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "    model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])\n",
    "\n",
    "    model.save('model/transformer16_cnn_fold'+str(fold)+'.h5')\n",
    "    with open('model/transformer16_cnn_fold'+str(fold)+'.json', 'w') as fp:\n",
    "        json.dump(model.history.history, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413442fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1d2ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "enc_seq_length = 600  # Maximum length of the input sequence\n",
    "dec_seq_length = enc_seq_length  # Maximum length of the target sequence\n",
    "\n",
    "fold=1\n",
    "data_all=pd.read_csv('data/fold'+str(fold)+'.csv')\n",
    "data_train=data_all[data_all['set']=='train']\n",
    "data_valid=data_all[data_all['set']=='valid']\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_valid = data_valid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X_train=encode_padding(data_train, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_train=data_train['label']\n",
    "X_valid=encode_padding(data_valid, col='seq', seq_len=enc_seq_length, padding=CFG.feature_pad_end, channel=1)\n",
    "y_valid=data_valid['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58c3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 4  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 128  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c159e40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1342ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa50e004",
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "              batch_size=64, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3139220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import LRScheduler, TransformerModel, encode_padding\n",
    "\n",
    "enc_vocab_size = 5 # Vocabulary size for the encoder\n",
    "dec_vocab_size = enc_vocab_size # Vocabulary size for the decoder\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 32  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 1  # Number of layers in the encoder stack\n",
    "dropout_rate = 0.1\n",
    "\n",
    "word_embedding_layer = Embedding(input_dim=enc_vocab_size, output_dim=d_model)\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "#outputs = training_model(inputs, training=True)\n",
    "#outputs = K.max(outputs,axis=-1)\n",
    "outputs = word_embedding_layer(inputs)\n",
    "outputs = Conv1D(activation=\"relu\", input_shape=(enc_seq_length, d_model), filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = Conv1D(activation=\"relu\", filters=128, kernel_size=8)(outputs)\n",
    "outputs = MaxPooling1D()(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(32)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "outputs = Dropout(0.1)(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3b3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length,\n",
    "                                      h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = K.max(outputs,axis=-1)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "          keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "          keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "          keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "          keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "          keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "          keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "    ]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32b354",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_split = GroupShuffleSplit(test_size=.20, n_splits=2)\n",
    "\n",
    "while True:\n",
    "    split = valid_split.split(data, y, groups=g)\n",
    "    train_inds, valid_inds = next(split)\n",
    "    if len(train_inds)/len(valid_inds)>3:\n",
    "        break\n",
    "    \n",
    "data_train=data.iloc[train_inds,:]\n",
    "data_valid=data.iloc[valid_inds,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac98d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b28b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e94f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_pos=data_train[data_train.label==1]\n",
    "data_train_pos_new = data_train_pos.sample(n=data_train.shape[0], random_state=1, replace=True)\n",
    "\n",
    "for i in range(data_train_pos_new.shape[0]):\n",
    "    r=random.uniform(0, 1)\n",
    "    if r>0.75:\n",
    "        tmp=random.sample([-1, -2, -3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][:tmp[0]]\n",
    "    if r >0.5 and r <=0.75:\n",
    "        tmp=random.sample([1, 2, 3], k=1)\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i][tmp[0]:]\n",
    "    if r >0.25 and r <=0.5:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=data_train_pos_new.seq.iloc[i] + tmp\n",
    "    else:\n",
    "        tmp=''.join(np.random.choice(add_on, size=random.sample([1,2,3],k=1), replace=True))\n",
    "        data_train_pos_new.seq.iloc[i]=tmp + data_train_pos_new.seq.iloc[i]\n",
    "\n",
    "data_train_all = pd.concat([data_train, data_train_pos_new])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7001487b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all = data_train_all.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8503dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_all['set']='train'\n",
    "data_valid['set']='valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5129df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b09148",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = pd.concat([data_train_all, data_valid])\n",
    "data_all.reset_index(drop=True)\n",
    "data_all['id']=str(fold)+'_'+data_all.index.astype(str)\n",
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36684553",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all.to_csv('data/fold'+str(fold)+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101e335",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=one_hot_encode_padding(data_train_all, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_train=data_train_all['label']\n",
    "X_valid=one_hot_encode_padding(data_valid, col='seq', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)\n",
    "y_valid=data_valid['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7e219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508b9cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(y_valid)\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(y_valid)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a9d5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9089e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = train_model(X_train, y_train, (X_valid, y_valid), channel=CFG.channel,\n",
    "                   nb_epoch=10, border_mode='same',\n",
    "                   inp_len=CFG.feature_seq_len, nodes=40, layers=5, nbr_filters=120, filter_len=8, dropout1=0,\n",
    "                   dropout2=0, dropout3=0.2, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca0abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2985f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/IRESbase_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaaaa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase=pd.read_csv('data/RF00031.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test=one_hot_encode_padding(IRESbase, col='IRES.Sequence', seq_len=CFG.feature_seq_len, padding=CFG.feature_pad_end, channel=CFG.channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bcbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea520987",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_test>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487c8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e770db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ba5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e79d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7b53f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 8  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 8  # Dimensionality of the linearly projected values\n",
    "d_ff = 8  # Dimensionality of the inner fully connected layer\n",
    "d_model = 8  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    " \n",
    "input_seq = np.random.random((batch_size, input_seq_length))\n",
    " \n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c86327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28971aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda \n",
    "device = cuda.get_current_device()\n",
    "device.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2c2bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_padding(df, col='utr', seq_len=50, padding='5end', channel=1):\n",
    "    # 5end padding means pad the left end (5' end) if sequence length < seq_len; keep the seq_len right end (3' end) if  sequence length > seq_len\n",
    "    # 3end padding means pad the right end (3' end) if sequence length < seq_len; keep the seq_len left end (5' end) if  sequence length > seq_len\n",
    "    # Dictionary returning one-hot encoding of nucleotides. \n",
    "    nuc_d = {'a':[1],'c':[2],'g':[3],'t':[4], 'n':[0], '(':[5],')':[6],'.':[7]}\n",
    "    \n",
    "    # Creat empty matrix.\n",
    "    vectors=np.zeros([len(df),seq_len,channel])\n",
    "    \n",
    "    # Iterate through UTRs and one-hot encode\n",
    "    for i,seq in enumerate(df[col]):\n",
    "        if(isinstance(seq, str)):\n",
    "            if(padding=='3end'):\n",
    "                seq=seq[:min(len(seq),seq_len)]\n",
    "            if(padding=='5end'):\n",
    "                seq=seq[max(0,(len(seq)-seq_len)):len(seq)]\n",
    "            seq = seq.lower()\n",
    "            a = np.array([nuc_d[x] for x in seq])\n",
    "            if(padding=='5end'):\n",
    "                vectors[i, (seq_len-len(seq)):seq_len] = a\n",
    "            if(padding=='3end'):\n",
    "                vectors[i, :len(seq)] = a\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4bef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=np.squeeze(encode_padding(data_train_all, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_train=data_train_all['label']\n",
    "X_valid=np.squeeze(encode_padding(data_valid, col='seq', seq_len=200, padding=CFG.feature_pad_end, channel=1))\n",
    "y_valid=data_valid['label']\n",
    "#m_valid=np.squeeze(tf.math.equal(X_valid, 0))*1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23b7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66c625c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654ec92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_vocab_size = 64 # Vocabulary size for the encoder\n",
    "dec_vocab_size = 64 # Vocabulary size for the decoder\n",
    " \n",
    "enc_seq_length = 200  # Maximum length of the input sequence\n",
    "dec_seq_length = 200  # Maximum length of the target sequence\n",
    "\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 32  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 32  # Dimensionality of the linearly projected values\n",
    "d_ff = 64  # Dimensionality of the inner fully connected layer\n",
    "d_model = 64  # Dimensionality of the model sub-layers' outputs\n",
    "n = 3  # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "# Create model\n",
    "training_model = TransformerModel(enc_vocab_size, dec_vocab_size, enc_seq_length, dec_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(enc_seq_length,))\n",
    "outputs = training_model(inputs, training=True)\n",
    "outputs = Flatten()(outputs)\n",
    "outputs = Dense(1)(outputs)\n",
    "outputs = Activation('sigmoid')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fabe95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dd505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b86ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "class LRScheduler(LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000, **kwargs):\n",
    "        super(LRScheduler, self).__init__(**kwargs)\n",
    " \n",
    "        self.d_model = cast(d_model, float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    " \n",
    "    def __call__(self, step_num):\n",
    " \n",
    "        # Linearly increasing the learning rate for the first warmup_steps, and decreasing it thereafter\n",
    "        arg1 = step_num ** -0.5\n",
    "        arg2 = step_num * (self.warmup_steps ** -1.5)\n",
    " \n",
    "        return (self.d_model ** -0.5) * math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71127cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(LRScheduler(d_model), beta_1=0.9, beta_2=0.999, epsilon=1e-08) \n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(thresholds=0.5, name='tp'),\n",
    "      keras.metrics.FalsePositives(thresholds=0.5,name='fp'),\n",
    "      keras.metrics.TrueNegatives(thresholds=0.5,name='tn'),\n",
    "      keras.metrics.FalseNegatives(thresholds=0.5,name='fn'), \n",
    "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "]\n",
    "\n",
    "model.compile(loss=BinaryFocalLoss(gamma=2), metrics=METRICS, optimizer=adam)\n",
    "\n",
    "es = EarlyStopping(monitor='val_prc', mode='max', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_valid, y_valid),\n",
    "          batch_size=256, epochs=10, verbose=1, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54840ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76178b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cd3468",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.build_graph().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32607a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(IRESbase['IRES.Sequence'].to_numpy(), \n",
    "                                              value='N', padding='post', truncating='post', maxlen=600, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa379d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.layers.TextVectorization('ATCG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1911f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a3ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_data = [\"A\", \"C\", \"G\", \"T\"]\n",
    "max_len = CFG.feature_seq_len\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    " max_tokens=max_features,\n",
    " output_mode='int',\n",
    " output_sequence_length=max_len,\n",
    " vocabulary=vocab_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "t  = Tokenizer(num_words=5,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=False, char_level=True, oov_token=None,\n",
    "    document_count=0)\n",
    "fit_text = \"ACGTN\"\n",
    "t.fit_on_texts(fit_text)\n",
    "\n",
    "test_text = \"NCGTA\"\n",
    "sequences = t.texts_to_sequences(test_text)\n",
    "\n",
    "print(\"sequences : \",sequences,'\\n')\n",
    "\n",
    "print(\"word_index : \", t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc052b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_valid['seq'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f9e071",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=t.texts_to_sequences(data_valid['seq'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404da5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        import numpy as np\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0949bca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2dbdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.squeeze(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd56201",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IRESbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ec76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "with open('nucl.json') as f:\n",
    "    contents = f.readlines()\n",
    "    t = tokenizer_from_json(contents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa51e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc466f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_tensorflow2_p36",
   "language": "python",
   "name": "conda_amazonei_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b07a4092d17fabcc770aac63fd8b04ec6822179f9cd74920d0d99ada38e73abc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
